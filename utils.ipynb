{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9474c7f0-10ee-401f-bf50-619a6d2205fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3f04eb-7a4c-49a9-aa49-7800f5d002de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage():\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a53ee-4bb1-4f90-8594-de69affbe70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(log_path):\n",
    "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
    "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
    "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
    "    Example:\n",
    "    ```\n",
    "    logging.info(\"Starting training...\")\n",
    "    ```\n",
    "    Args:\n",
    "        log_path: (string) where to log\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138179bb-cf35-43d7-884f-d85f35308841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint):\n",
    "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
    "    checkpoint + 'best.pth.tar'\n",
    "    Args:\n",
    "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
    "        is_best: (bool) True if it is the best model seen till now\n",
    "        checkpoint: (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
    "        os.mkdir(checkpoint)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f545e0-7f78-4b5c-9a40-5ed47457dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    # model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07ec65-b557-4c62-93f1-c73491ef01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path, optimizer=None, resume=False, \n",
    "               lr=None, lr_step=None):\n",
    "    start_epoch = 0\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "      model = model.module\n",
    "    checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    #print('loaded {}, epoch {}'.format(model_path, checkpoint['epoch']))\n",
    "    state_dict_ = checkpoint['state_dict']\n",
    "    state_dict = {}\n",
    "    \n",
    "    # convert data_parallal to model\n",
    "    for k in state_dict_:\n",
    "        state_dict[k] = state_dict_[k]\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # check loaded parameters and created model parameters\n",
    "    for k in state_dict:\n",
    "        if k in model_state_dict:\n",
    "            if state_dict[k].shape != model_state_dict[k].shape:\n",
    "                print('Skip loading parameter {}, required shape{}, '\\\n",
    "                    'loaded shape{}.'.format(k, model_state_dict[k].shape, state_dict[k].shape))\n",
    "                state_dict[k] = model_state_dict[k]\n",
    "        else:\n",
    "            print('Drop parameter {}.'.format(k))\n",
    "    for k in model_state_dict:\n",
    "        if not (k in state_dict):\n",
    "            print('No param {}.'.format(k))\n",
    "            state_dict[k] = model_state_dict[k]\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
