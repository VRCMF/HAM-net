{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ca288-7d49-4b28-a652-4ffea160d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from tqdm import trange\n",
    "from transformers import BertTokenizer, BertModel, BertForTokenClassification, BertConfig\n",
    "import pytorch_pretrained_bert\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a827e6-9fe5-4a50-b960-e849ba8b8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./data_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29651806-f9fa-430a-9d33-bf30019bf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./options.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12652c-189f-405d-b7e8-55177887339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41fffb-43ad-4aa7-972a-a8603cd4f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1d312-b583-450f-973b-5aa566d92cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e4c94-d7d9-448c-937e-1bb99a706cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_mask(x, pad_id):\n",
    "    bert_mask = (x != pad_id).float()\n",
    "    return bert_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f2218-17ef-4c91-805c-e56d00d185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_iterator, params, mark='Eval', verbose=False):\n",
    "    \"\"\"Evaluate the model on `steps` batches.\"\"\"\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    idx2tag = params.idx2tag\n",
    "    idx2word = BertTokenizer.from_pretrained(params.pretrained_model)\n",
    "    #else:\n",
    "    #  idx2word = pytorch_pretrained_bert.BertTokenizer.from_pretrained(params.pretrain_path)\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    word_data = []\n",
    "    pt_list = []\n",
    "    doc_string = \"\"\n",
    "    # a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    for i in range(params.eval_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "          model = model.module\n",
    "        batch_data, batch_tags = next(data_iterator)\n",
    "        batch_masks = batch_data.gt(0)\n",
    "        #x_mask = make_bert_mask(batch_data, 0)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch_data, \"attention_mask\": batch_masks, \"labels\": batch_tags}\n",
    "        #print(x_mask.shape)\n",
    "        if params.pretrain:\n",
    "          outputs = model(**inputs) \n",
    "        else:\n",
    "          outputs = model(**inputs)\n",
    "        #print(outputs)\n",
    "        tmp_eval_loss, logits = outputs[0], outputs[1]\n",
    "        #weights = outputs[2]\n",
    "        if params.decoder == 'crf':\n",
    "          tags = model.crf.decode(logits, batch_masks)\n",
    "          batch_output = torch.squeeze(tags, 0).detach().cpu().numpy()\n",
    "        elif params.decoder == 'linear':\n",
    "          tags = np.argmax(logits.detach().cpu().numpy(), axis=2).tolist()\n",
    "          batch_output = tags\n",
    "          #print(outputs)\n",
    "        # choose probability of target class\n",
    "        \n",
    "        #a0 = tags.detach().cpu().numpy()[0]\n",
    "        #b0 = weights.detach().cpu().numpy()\n",
    "        #tmp_\n",
    "        #for ii in range(len(a0)):\n",
    "        #  b0[ii, a0[ii]]\n",
    "        #print(len(tags))\n",
    "        #o = model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n",
    "        #print(tag_seq)\n",
    "        tmp_eval_loss = tmp_eval_loss.mean()\n",
    "        \n",
    "        loss_avg.update(tmp_eval_loss.item())\n",
    "        \n",
    "        #batch_output = model(batch_data, token_type_ids=None, attention_mask=batch_masks).logits  # shape: (batch_size, max_len, num_labels)\n",
    "        \n",
    "        \n",
    "        batch_tags = batch_tags.detach().cpu().numpy()\n",
    "        batch_data = batch_data.detach().cpu().numpy()\n",
    "        #print(batch_data)\n",
    "        #pred_tags.extend([idx2tag.get(idx) for indices in np.argmax(batch_output, axis=2) for idx in indices])\n",
    "        word_data.extend([idx2word.convert_ids_to_tokens(int(idx)) for indices in batch_data for idx in indices])\n",
    "        pred_tags.extend([idx2tag.get(idx) for indices in batch_output for idx in indices])\n",
    "        true_tags.extend([idx2tag.get(idx) for indices in batch_tags for idx in indices])\n",
    "        #doc_string += \" \".join(pred_tags)\n",
    "        #doc_string += '\\n'\n",
    "        #pred_list = list(chain.from_iterable(tag_seq))\n",
    "        #pred_tags += pred_list\n",
    "        \n",
    "        #batch_lens = 16\n",
    "        #true_list = list(chain.from_iterable([sublist[:batch_lens.tolist()[b]] for b, sublist in enumerate(batch_y.tolist())]))\n",
    "        #print(len(pred_tags))\n",
    "        #print(len(true_tags))\n",
    "        #if i == 0:\n",
    "        #  a1 = \", \".join(pred_tags)\n",
    "        #  a2 = \", \".join(true_tags)\n",
    "        #  a3 = \", \".join(word_data)\n",
    "        #  a = a1 + '\\n' + a2+ '\\n' + a3\n",
    "        #  f_p = \"/dbfs/FileStore/shared_uploads/hus45338967@hustietoallas.fi/pre/pred_crf_{}_{}.txt\".format(params.epoch_record, i)\n",
    "        #  with open(f_p, \"w\") as f:\n",
    "        #    f.write(a)\n",
    "        \n",
    "\n",
    "        \n",
    "    #f_p = \"../pred_crf_{}_{}.txt\".format(params.epoch_record, i)\n",
    "    #with open(f_p, \"w\") as f:\n",
    "    #  f.write(doc_string)\n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "\n",
    "    # logging loss, f1 and report\n",
    "    metrics = {}\n",
    "    p, r, f1 = eval_score(true_tags, pred_tags)\n",
    "    metrics['loss'] = loss_avg()\n",
    "    metrics['f1'] = f1\n",
    "    metrics['prec'] = p\n",
    "    metrics['rec'] = r\n",
    "    rp = classification_report(true_tags, pred_tags)\n",
    "    metrics_str = \"; \".join(\"{}: {:05.2f}\".format(k, v) for k, v in metrics.items())\n",
    "    #logging.info(\"- {} metrics: \".format(mark) + metrics_str)\n",
    "    print(\"- {} metrics: \".format(mark) + metrics_str)\n",
    "\n",
    "    if verbose:\n",
    "        report = classification_report(true_tags, pred_tags)\n",
    "        #logging.info(report)\n",
    "        print(report)\n",
    "    return metrics, rp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4eba0-8e19-4c4a-b865-57b45a157beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_and_test(model, test_data, optimizer, scheduler, args, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth')\n",
    "        #logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        print(\"Restoring parameters from {}\".format(restore_path))\n",
    "        load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, args.n_epochs + 1):\n",
    "        # Run one epoch\n",
    "        args.epoch_record = epoch\n",
    "        #logging.info(\"Epoch {}/{}\".format(epoch, args.n_epochs))\n",
    "        print(\"Epoch {}/{}\".format(epoch, args.n_epochs))\n",
    "        args.test_steps = args.test_size // args.batch_size\n",
    "\n",
    "        test_data_iterator = data_loader.data_iterator(test_data, shuffle=False)\n",
    "\n",
    "        args.eval_steps = args.test_steps\n",
    "        test_metrics, test_rp = evaluate(model, test_data_iterator, args, mark='Test')\n",
    "        \n",
    "\n",
    "        # Save weights of the network\n",
    "        #model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        optimizer_to_save = optimizer.optimizer if args.fp16 else optimizer\n",
    "        print(test_metrics)\n",
    "        print(test_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e43d0-6840-479a-a7b6-371446c14fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  args = args_parser()\n",
    "  \n",
    "  args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  args.gpus = torch.cuda.device_count()\n",
    "  args.n_epochs = 1\n",
    "  random.seed(args.seed)\n",
    "  torch.manual_seed(args.seed)\n",
    "  \n",
    "  if args.gpus > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)  # set random seed for all GPUs\n",
    "  args.seed = args.seed\n",
    "  #args.batch_size = 1\n",
    "  # Set the logger\n",
    "  #logging.info(\"device: {}, gpus: {}\".format(args.device, args.gpus))\n",
    "  print(\"device: {}, gpus: {}\".format(args.device, args.gpus))\n",
    "\n",
    "  # Create the input data pipeline\n",
    "  #logging.info(\"Loading the datasets...\")\n",
    "  print(\"Loading the datasets...\")\n",
    "  \n",
    "  # Initialize the DataLoader\n",
    "  data_loader = DataLoader(args.data_dir, args.bert_model_dir, args, token_pad_idx=0)\n",
    "  \n",
    "  # Load training data and test data\n",
    "  test_data = data_loader.load_data('test')\n",
    "  \n",
    "  args.test_size = test_data['size']\n",
    "  \n",
    "  #config = BertConfig.from_json_file('../bert-base-finnish-uncased-v1/tokenizer.json')\n",
    "  #model = BertForTokenClassification.from_pretrained(args.pretrained_model, num_labels=len(args.tag2idx))\n",
    "  config = BertConfig.from_pretrained(args.pretrained_model, num_labels=len(args.tag2idx))\n",
    "  if args.pretrain == True:\n",
    "    config.pretrain = args.pretrain\n",
    "    config.pretrain_path = args.pretrain_path\n",
    "  config.loss_type = args.loss_type\n",
    "  if args.decoder == 'linear':\n",
    "    if args.pretrain == False:\n",
    "      model = BERT_Linear(config)\n",
    "    else:\n",
    "      model = BERT_Linear_pre(config)\n",
    "  elif args.decoder == 'crf':\n",
    "    if args.pretrain == False:\n",
    "      model = BERT_CRF(config)\n",
    "    else:\n",
    "      model = BERT_CRF_pre(config)\n",
    "  elif args.decoder == 'lan':\n",
    "    config.drop_rate = 0\n",
    "    config.head_num = 1\n",
    "    model = BERT_LAN(config)\n",
    "  else:\n",
    "    raise RuntimeError(\"wrong model name\")\n",
    "  model.to(args.device)\n",
    "  #model._init_weights()\n",
    "  if args.gpus > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "  #checkpoint = torch.load('/../best.pth.tar')\n",
    "  model = load_model(model, '../linear_no_pretrain_lsr_kir_oper/best.pth.tar')\n",
    "  #model.load_state_dict(checkpoint['state_dict'])\n",
    "  #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "  optimizer = None\n",
    "  scheduler = None#LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(1 + 0.05*epoch))\n",
    "  \n",
    "  # Train and evaluate the model\n",
    "  #logging.info(\"Starting training for {} epoch(s)\".format(args.n_epochs))\n",
    "  print(\"Starting training for {} epoch(s)\".format(args.n_epochs))\n",
    "  train_and_evaluate_and_test(model, test_data, optimizer, scheduler, args, args.model_dir, args.restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c3f8d-4fac-4161-8ab6-e74a4409a91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
