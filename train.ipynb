{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff4ebae-22a6-426c-add4-9b0a55695cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from tqdm import trange\n",
    "from transformers import BertTokenizer, BertModel, BertForTokenClassification, BertConfig\n",
    "#import pytorch_pretrained_bert\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70670d56-9151-4ab4-82fc-dff3914ce731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./data_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a10e38-8833-423c-b668-767760abd1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b4209d-6876-4a93-b1e4-614d5252eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./options.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8aeae7e-25f8-4cc9-85fc-15c97e28a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7e488f-08ac-44c3-8c36-9bbf716a63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05e59f7-4fc0-44ca-91e8-4dce0659330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_mask(x, pad_id):\n",
    "    bert_mask = (x != pad_id).float()\n",
    "    return bert_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5200f2-dda8-4821-9793-2a7ea52f790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_iterator, optimizer, scheduler, args):\n",
    "    \"\"\"Train the model on `steps` batches\"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_list = []\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(args.train_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "          model = model.module\n",
    "        batch_data, batch_tags = next(data_iterator)\n",
    "        #print(batch_data.shape)\n",
    "        batch_masks = make_bert_mask(batch_data, 0)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch_data, \"attention_mask\": batch_masks, \"labels\": batch_tags}\n",
    "        #print(x_mask.shape)\n",
    "        if args.pretrain:\n",
    "          outputs = model(**inputs)#model(batch_data, batch_masks, batch_tags) \n",
    "        else:\n",
    "          outputs = model(**inputs)\n",
    "        #print(outputs)\n",
    "        #model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n",
    "        #loss = -1 * log_likelihood\n",
    "        #print(outputs)\n",
    "        # print(outputs)\n",
    "        # print(outputs[0])\n",
    "        \n",
    "        # linear\n",
    "        # loss = outputs[0][0].mean()# to average on multi-gpu\n",
    "        # crf\n",
    "        loss = outputs[0].mean()\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #if args.fp16:\n",
    "        #    optimizer.backward(loss)\n",
    "        #else:\n",
    "        #   loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1709d2cc-a0fa-4fdc-940e-03d26a1b4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_iterator, params, mark='Eval', verbose=False):\n",
    "    \"\"\"Evaluate the model on `steps` batches.\"\"\"\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    idx2tag = params.idx2tag\n",
    "    #if params.pretrain == False:\n",
    "    idx2word = BertTokenizer.from_pretrained(params.pretrained_model)\n",
    "    #else:\n",
    "    #  idx2word = pytorch_pretrained_bert.BertTokenizer.from_pretrained(params.pretrain_path)\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    word_data = []\n",
    "\n",
    "    # a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    for i in range(params.eval_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "          model = model.module\n",
    "        batch_data, batch_tags = next(data_iterator)\n",
    "        #batch_masks = batch_data.gt(0)\n",
    "        batch_masks = make_bert_mask(batch_data, 0)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch_data, \"attention_mask\": batch_masks, \"labels\": batch_tags}\n",
    "        #print(x_mask.shape)\n",
    "        if params.pretrain:\n",
    "          outputs = model(**inputs)#model(batch_data, batch_masks, batch_tags) \n",
    "        else:\n",
    "          outputs = model(**inputs)\n",
    "        #print(outputs)\n",
    "        tmp_eval_loss, logits = outputs[0], outputs[1]\n",
    "        if params.decoder == 'crf':\n",
    "          tags = model.crf.decode(logits, batch_masks)\n",
    "          batch_output = torch.squeeze(tags, 0).detach().cpu().numpy()\n",
    "        elif params.decoder == 'linear':\n",
    "          tags = np.argmax(logits.detach().cpu().numpy(), axis=2).tolist()\n",
    "          batch_output = tags\n",
    "          #print(outputs)\n",
    "        #print(tags)\n",
    "        #print(len(tags))\n",
    "        #o = model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n",
    "        #print(tag_seq)\n",
    "        # print(tmp_eval_loss)\n",
    "        # linear\n",
    "        tmp_eval_loss = tmp_eval_loss.mean()\n",
    "        # crf\n",
    "        tmp_eval_loss = tmp_eval_loss.mean()\n",
    "        \n",
    "        loss_avg.update(tmp_eval_loss.item())\n",
    "        \n",
    "        #batch_output = model(batch_data, token_type_ids=None, attention_mask=batch_masks).logits  # shape: (batch_size, max_len, num_labels)\n",
    "        \n",
    "        \n",
    "        batch_tags = batch_tags.detach().cpu().numpy()\n",
    "        batch_data = batch_data.detach().cpu().numpy()\n",
    "\n",
    "        #pred_tags.extend([idx2tag.get(idx) for indices in np.argmax(batch_output, axis=2) for idx in indices])\n",
    "        word_data.extend([idx2word.convert_ids_to_tokens(int(idx)) for indices in batch_data for idx in indices])\n",
    "        pred_tags.extend([idx2tag.get(idx) for indices in batch_output for idx in indices])\n",
    "        true_tags.extend([idx2tag.get(idx) for indices in batch_tags for idx in indices])\n",
    "        #pred_list = list(chain.from_iterable(tag_seq))\n",
    "        #pred_tags += pred_list\n",
    "        \n",
    "        #batch_lens = 16\n",
    "        #true_list = list(chain.from_iterable([sublist[:batch_lens.tolist()[b]] for b, sublist in enumerate(batch_y.tolist())]))\n",
    "        #print(len(pred_tags))\n",
    "        #print(len(true_tags))\n",
    "        #if i == 0:\n",
    "        #  a1 = \", \".join(pred_tags)\n",
    "        #  a2 = \", \".join(true_tags)\n",
    "        #  a3 = \", \".join(word_data)\n",
    "        #  a = a1 + '\\n' + a2+ '\\n' + a3\n",
    "        #  f_p = \"/dbfs/FileStore/shared_uploads/hus45338967@hustietoallas.fi/pre/pred_crf_{}_{}.txt\".format(params.epoch_record, i)\n",
    "        #  with open(f_p, \"w\") as f:\n",
    "        #    f.write(a)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "\n",
    "    # logging loss, f1 and report\n",
    "    metrics = {}\n",
    "    p, r, f1 = eval_score(true_tags, pred_tags)\n",
    "    metrics['loss'] = loss_avg()\n",
    "    metrics['f1'] = f1\n",
    "    metrics['prec'] = p\n",
    "    metrics['rec'] = r\n",
    "    rp = classification_report(true_tags, pred_tags)\n",
    "    metrics_str = \"; \".join(\"{}: {:05.2f}\".format(k, v) for k, v in metrics.items())\n",
    "    #logging.info(\"- {} metrics: \".format(mark) + metrics_str)\n",
    "    print(\"- {} metrics: \".format(mark) + metrics_str)\n",
    "\n",
    "    if verbose:\n",
    "        report = classification_report(true_tags, pred_tags)\n",
    "        #logging.info(report)\n",
    "        print(report)\n",
    "    return metrics, rp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b24269-b8c3-41a5-9b33-16bf58203c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_and_test(model, train_data, val_data, test_data, optimizer, scheduler, args, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth')\n",
    "        #logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        print(\"Restoring parameters from {}\".format(restore_path))\n",
    "        load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_f1 = 0.0\n",
    "    best_test_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, args.n_epochs + 1):\n",
    "        # Run one epoch\n",
    "        args.epoch_record = epoch\n",
    "        #logging.info(\"Epoch {}/{}\".format(epoch, args.n_epochs))\n",
    "        print(\"Epoch {}/{}\".format(epoch, args.n_epochs))\n",
    "\n",
    "        # Compute number of batches in one epoch\n",
    "        args.train_steps = args.train_size // args.batch_size\n",
    "        args.val_steps = args.val_size // args.batch_size\n",
    "        args.test_steps = args.test_size // args.batch_size\n",
    "\n",
    "        # data iterator for training\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, shuffle=True)\n",
    "        # Train for one epoch on training set\n",
    "        #!!!!\n",
    "        train(model, train_data_iterator, optimizer, scheduler, args)\n",
    "\n",
    "        # data iterator for evaluation\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, shuffle=False)\n",
    "        val_data_iterator = data_loader.data_iterator(val_data, shuffle=False)\n",
    "        test_data_iterator = data_loader.data_iterator(test_data, shuffle=False)\n",
    "\n",
    "        # Evaluate for one epoch on training set and validation set\n",
    "        args.eval_steps = args.train_steps\n",
    "        train_metrics, train_rp = evaluate(model, train_data_iterator, args, mark='Train')\n",
    "        args.eval_steps = args.val_steps\n",
    "        val_metrics, val_rp = evaluate(model, val_data_iterator, args, mark='Val')\n",
    "        args.eval_steps = args.test_steps\n",
    "        test_metrics, test_rp = evaluate(model, test_data_iterator, args, mark='Test')\n",
    "        \n",
    "        val_f1 = val_metrics['f1']\n",
    "        #improve_f1 = val_f1 - best_val_f1\n",
    "        improve_f1 = test_metrics['f1'] - best_test_f1\n",
    "        # Save weights of the network\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        optimizer_to_save = optimizer.optimizer if args.fp16 else optimizer\n",
    "        save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model_to_save.state_dict(),\n",
    "                               'optim_dict': optimizer_to_save.state_dict()},\n",
    "                               is_best=improve_f1>0,\n",
    "                               checkpoint=model_dir)\n",
    "        if improve_f1 > 0:\n",
    "            #logging.info(\"- Found new best F1\")\n",
    "            print(\"- Found new best F1\")\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            best_test_f1 = test_metrics['f1']\n",
    "            best_val = \"; \".join(\"{}: {:05.2f}\".format(k, v) for k, v in val_metrics.items())\n",
    "            best_test = \"; \".join(\"{}: {:05.2f}\".format(k, v) for k, v in test_metrics.items())\n",
    "            best_rp = test_rp\n",
    "            if improve_f1 < args.patience:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping and logging best f1\n",
    "        if (patience_counter >= args.patience_num and epoch > args.n_epochs) or epoch == args.n_epochs:\n",
    "            #logging.info(\"Best val f1: {:05.2f}\".format(best_val_f1))\n",
    "            print(\"Best val score - eval metrics: \" + best_val)\n",
    "            print(\"Best test score - test metrics: \" + best_test)\n",
    "            print(best_rp)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd127-d623-4803-8b8b-98665bb157c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda, gpus: 1\n",
      "Loading the datasets...\n",
      "Starting training for 10 epoch(s)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17206/17206 [43:24<00:00,  6.61it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 00.43; f1: 49.76; prec: 89.84; rec: 34.40\n",
      "- Val metrics: loss: 00.46; f1: 48.13; prec: 87.93; rec: 33.14\n",
      "- Test metrics: loss: 03.14; f1: 07.36; prec: 32.39; rec: 04.15\n",
      "Checkpoint Directory does not exist! Making directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/nergpu/code/Users/Wei/model/linear_pretrain_ce\n",
      "- Found new best F1\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17206/17206 [42:49<00:00,  6.70it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 00.37; f1: 57.19; prec: 87.74; rec: 42.42\n",
      "- Val metrics: loss: 00.47; f1: 51.89; prec: 82.14; rec: 37.92\n",
      "- Test metrics: loss: 03.42; f1: 08.76; prec: 32.16; rec: 05.07\n",
      "- Found new best F1\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17206/17206 [42:50<00:00,  6.69it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 00.32; f1: 63.93; prec: 86.01; rec: 50.86\n",
      "- Val metrics: loss: 00.51; f1: 52.84; prec: 75.29; rec: 40.71\n",
      "- Test metrics: loss: 03.76; f1: 10.13; prec: 32.67; rec: 05.99\n",
      "- Found new best F1\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 5144/17206 [12:49<30:00,  6.70it/s, loss=0.350]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  args = args_parser()\n",
    "  \n",
    "  args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  args.gpus = torch.cuda.device_count()\n",
    "  \n",
    "  random.seed(args.seed)\n",
    "  torch.manual_seed(args.seed)\n",
    "  \n",
    "  if args.gpus > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)  # set random seed for all GPUs\n",
    "  args.seed = args.seed\n",
    "  \n",
    "  # Set the logger\n",
    "  #logging.info(\"device: {}, gpus: {}\".format(args.device, args.gpus))\n",
    "  print(\"device: {}, gpus: {}\".format(args.device, args.gpus))\n",
    "\n",
    "  # Create the input data pipeline\n",
    "  #logging.info(\"Loading the datasets...\")\n",
    "  print(\"Loading the datasets...\")\n",
    "  \n",
    "  # Initialize the DataLoader\n",
    "  data_loader = DataLoader(args.data_dir, args.bert_model_dir, args, token_pad_idx=0)\n",
    "  \n",
    "  # Load training data and test data\n",
    "  train_data = data_loader.load_data('train')\n",
    "  val_data = data_loader.load_data('dev')\n",
    "  test_data = data_loader.load_data('test')\n",
    "  \n",
    "  args.train_size = train_data['size']\n",
    "  args.val_size = val_data['size']\n",
    "  args.test_size = test_data['size']\n",
    "  \n",
    "  #config = BertConfig.from_json_file('../')\n",
    "  #model = BertForTokenClassification.from_pretrained(args.pretrained_model, num_labels=len(args.tag2idx))\n",
    "  config = BertConfig.from_pretrained(args.pretrained_model, num_labels=len(args.tag2idx))\n",
    "  if args.pretrain == True:\n",
    "    config.pretrain = args.pretrain\n",
    "    config.pretrain_path = args.pretrain_path\n",
    "  config.loss_type = args.loss_type\n",
    "  if args.decoder == 'linear':\n",
    "    if args.pretrain == False:\n",
    "      model = BERT_Linear(config)\n",
    "    else:\n",
    "      model = BERT_Linear_pre(config)\n",
    "  elif args.decoder == 'crf':\n",
    "    if args.pretrain == False:\n",
    "      model = BERT_CRF(config)\n",
    "    else:\n",
    "      model = BERT_CRF_pre(config)\n",
    "  elif args.decoder == 'lan':\n",
    "    config.drop_rate = 0\n",
    "    config.head_num = 1\n",
    "    model = BERT_LAN(config)\n",
    "  else:\n",
    "    raise RuntimeError(\"wrong model name\")\n",
    "  model.to(args.device)\n",
    "  #model._init_weights()\n",
    "  if args.gpus > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "  #model = model.cuda()\n",
    "  # Prepare optimizer\n",
    "  args.lr_layer_decay = 1.0\n",
    "  args.weight_decay = 0\n",
    "  if args.full_finetuning:\n",
    "    # param_optimizer = list(model.named_parameters())\n",
    "    # print([item[0] for item in param_optimizer])\n",
    "    # no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    # no_decay = ['bias', 'gamma', 'beta']\n",
    "    # optimizer_grouped_parameters = [\n",
    "    #     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "    #      'weight_decay_rate': 0.01},\n",
    "    #     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "    #      'weight_decay_rate': 0.0}\n",
    "    # ]\n",
    "    # print([item[0] for item in param_optimizer])\n",
    "    optimizer = AdamW(\n",
    "                [{'params':model.bert.embeddings.parameters(), 'lr': args.lr * args.lr_layer_decay ** (len(model.bert.encoder.layer) + 2)}]\n",
    "                +\n",
    "                [{'params': module_list_item.parameters(), 'lr': args.lr * args.lr_layer_decay ** (len(model.bert.encoder.layer) + 1 - index), } \n",
    "                for index, module_list_item in enumerate(model.bert.encoder.layer)]\n",
    "                +\n",
    "                [{'params':model.bert.pooler.parameters(), 'lr': args.lr * args.lr_layer_decay ** 1}]\n",
    "                +\n",
    "                [{'params':model.classifier.parameters(), 'lr': args.lr * args.lr_layer_decay ** 0}]\n",
    "                # +\n",
    "                # [{'params':model.crf.parameters(), 'lr': args.lr * args.lr_layer_decay ** 0}]\n",
    "            , weight_decay = args.weight_decay)\n",
    "  else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "    \n",
    "  # optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "  scheduler = None\n",
    "  # scheduler --> affect performance\n",
    "  # scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(1 + 0.05*epoch))\n",
    "  \n",
    "  # Train and evaluate the model\n",
    "  #logging.info(\"Starting training for {} epoch(s)\".format(args.n_epochs))\n",
    "  print(\"Starting training for {} epoch(s)\".format(args.n_epochs))\n",
    "  train_and_evaluate_and_test(model, train_data, val_data, test_data, optimizer, scheduler, args, args.model_dir, args.restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b625364-a553-4c38-899b-c33e2f4e2f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
