{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a0f21-84b7-4e1f-8cab-aee7615e0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertConfig\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ec9e1-8db1-4d76-a64f-df66a2e5d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./loss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994446f1-41f3-45f8-93c9-125de2ce8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./layer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cc601-fe81-4dcb-82ce-2319bcd74c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BERT_CRF, self).__init__(config)\n",
    "    \n",
    "    self.bert = AutoModel.from_pretrained(args.pretrained_model)\n",
    "    self.dropout = nn.Dropout(args.drop)\n",
    "    self.classifier = nn.Linear(768, len(args.tag2idx))\n",
    "    self.crf = CRF(num_tags = len(args.tag2idx), batch_first=True)\n",
    "    self.init_weights()\n",
    "  \n",
    "  def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "    #x_mask = self.make_bert_mask(x, 0)\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    #print(output.shape)\n",
    "    #emissions = self.position_wise_ff(output)\n",
    "    sequence_output = outputs[0]\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    logits = self.classifier(sequence_output)\n",
    "    #outputs = (logits,)\n",
    "    #log_likelihood = self.crf(emissions, tags, mask=x_mask.bool(), reduction='mean')\n",
    "    #sequence_of_tags =  self.crf.decode(emissions, mask=x_mask.bool())\n",
    "    loss = self.crf(emissions = logits, tags=labels, mask=attention_mask)\n",
    "    #outputs =(-1*loss,)+outputs\n",
    "    return (-1*loss, logits)\n",
    "    \n",
    "  #def init_model(self):\n",
    "  #  init_list = [self.classifier, self.crf]\n",
    "  #  for module in init_list:\n",
    "  #    for param in module.parameters():\n",
    "  #      if param.dim() > 1:\n",
    "  #        nn.init.xavier_uniform_(param)\n",
    "    #return log_likelihood, sequence_of_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005225c6-f1c7-446e-a169-9ca4538cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7eb5b-fa09-492b-a902-fdd1f7a92d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_pretrained_bert.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcbf566-0d8f-4842-b8d9-86cfacf05b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF_pre(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(BERT_CRF_pre, self).__init__()\n",
    "    \n",
    "    self.bert = pytorch_pretrained_bert.BertModel.from_pretrained(config.pretrain_path)\n",
    "    self.dropout = nn.Dropout(args.drop)\n",
    "    self.classifier = nn.Linear(768, len(args.tag2idx))\n",
    "    self.crf = CRF(num_tags = len(args.tag2idx), batch_first=True)\n",
    "    #self.init_weights()\n",
    "  \n",
    "  def forward(self,input_ids, attention_mask, labels):\n",
    "    #print('yes')\n",
    "    outputs = self.bert(input_ids, attention_mask=attention_mask, output_all_encoded_layers=False)\n",
    "    #print(output.shape)\n",
    "    #emissions = self.position_wise_ff(output)\n",
    "    sequence_output = outputs[0]\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    logits = self.classifier(sequence_output)\n",
    "    #outputs = (logits,)\n",
    "    #log_likelihood = self.crf(emissions, tags, mask=x_mask.bool(), reduction='mean')\n",
    "    #sequence_of_tags =  self.crf.decode(emissions, mask=x_mask.bool())\n",
    "    loss = self.crf(emissions = logits, tags=labels, mask=attention_mask)\n",
    "    #outputs =(-1*loss,)+outputs\n",
    "    return (-1*loss, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303997c-639f-446f-8ff8-243dce786b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Linear_pre(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(BERT_Linear_pre, self).__init__()\n",
    "    \n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = pytorch_pretrained_bert.BertModel.from_pretrained(config.pretrain_path)\n",
    "    \n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    self.loss_type = args.loss_type\n",
    "    \n",
    "    #self.init_weights()\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  def forward(self,input_ids, attention_mask, labels):\n",
    "        #return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, output_all_encoded_layers=False)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        #outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            assert self.loss_type in ['lsr', 'fl', 'ce']\n",
    "            if self.loss_type == 'lsr':\n",
    "                loss_fct = LabelSmoothingCrossEntropy()\n",
    "            elif self.loss_type == 'fl':\n",
    "                #loss_fct = FocalLoss()\n",
    "                loss_fct = FocalLoss()\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                #print(active_logits)\n",
    "                #print(active_labels)\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            #outputs = (loss,) + outputs\n",
    "        return (loss, logits)  # (loss), scores, (hidden_states), (attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f72044-6980-4f48-9185-6046c33e4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Linear(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BERT_Linear, self).__init__(config)\n",
    "    \n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config, add_pooling_layer=False)\n",
    "    \n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    self.loss_type = args.loss_type\n",
    "    \n",
    "    self.init_weights()\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "    \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "      \n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        #outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            assert self.loss_type in ['lsr', 'fl', 'ce']\n",
    "            if self.loss_type == 'lsr':\n",
    "                loss_fct = LabelSmoothingCrossEntropy()\n",
    "            elif self.loss_type == 'fl':\n",
    "                loss_fct = FocalLoss()\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            #outputs = (loss,) + outputs\n",
    "        return (loss, logits)  # (loss), scores, (hidden_states), (attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6cacc-9d48-40a4-8835-c7f066b86388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, num_heads=1, dropout_rate=0, gpu=True, causality=False):\n",
    "        '''Applies multihead attention.\n",
    "        Args:\n",
    "            num_units: A scalar. Attention size.\n",
    "            dropout_rate: A floating point number.\n",
    "            causality: Boolean. If true, units that reference the future are masked.\n",
    "            num_heads: An int. Number of heads.\n",
    "        '''\n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.causality = causality\n",
    "        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        if self.gpu:\n",
    "            self.Q_proj = self.Q_proj.cuda()\n",
    "            self.K_proj = self.K_proj.cuda()\n",
    "            self.V_proj = self.V_proj.cuda()\n",
    "\n",
    "\n",
    "        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, queries, keys, values,last_layer = False):\n",
    "        # keys, values: same shape of [N, T_k, C_k]\n",
    "        # queries: A 3d Variable with shape of [N, T_q, C_q]\n",
    "        # Linear projections\n",
    "        Q = self.Q_proj(queries)  # (N, T_q, C)\n",
    "        K = self.K_proj(keys)  # (N, T_q, C)\n",
    "        V = self.V_proj(values)  # (N, T_q, C)\n",
    "        # Split and concat\n",
    "        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        # Multiplication\n",
    "        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n",
    "        # Scale\n",
    "        outputs = outputs / (K_.size()[-1] ** 0.5)\n",
    "\n",
    "        # Activation\n",
    "        if last_layer == False:\n",
    "            outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n",
    "        # Query Masking\n",
    "        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n",
    "        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n",
    "        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n",
    "        outputs = outputs * query_masks\n",
    "        # Dropouts\n",
    "        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n",
    "        if last_layer == True:\n",
    "            return outputs\n",
    "        # Weighted sum\n",
    "        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d78604-b509-4e13-8e10-441303eb8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_LAN(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BERT_LAN, self).__init__(config)\n",
    "    \n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config, add_pooling_layer=False)\n",
    "    \n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = multihead_attention(768, num_heads=config.head_num, dropout_rate=config.drop_rate)\n",
    "    self.loss_type = args.loss_type\n",
    "    self.label_dim = 200\n",
    "    self.label_embedding = nn.Embedding(self.num_labels, self.label_dim)\n",
    "    \n",
    "    self.init_weights()\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "    \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        #print(outputs)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output, sequence_output, sequence_output)\n",
    "        #outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            assert self.loss_type in ['lsr', 'fl', 'ce']\n",
    "            if self.loss_type == 'lsr':\n",
    "                loss_fct = LabelSmoothingCrossEntropy()\n",
    "            elif self.loss_type == 'fl':\n",
    "                loss_fct = FocalLoss()\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            #outputs = (loss,) + outputs\n",
    "        return (loss, logits)  # (loss), scores, (hidden_states), (attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e2754-5f31-4215-a681-642c44e95dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, num_heads=1, dropout_rate=0, gpu=True, causality=False):\n",
    "        '''Applies multihead attention.\n",
    "        Args:\n",
    "            num_units: A scalar. Attention size.\n",
    "            dropout_rate: A floating point number.\n",
    "            causality: Boolean. If true, units that reference the future are masked.\n",
    "            num_heads: An int. Number of heads.\n",
    "        '''\n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.causality = causality\n",
    "        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        if self.gpu:\n",
    "            self.Q_proj = self.Q_proj.cuda()\n",
    "            self.K_proj = self.K_proj.cuda()\n",
    "            self.V_proj = self.V_proj.cuda()\n",
    "\n",
    "\n",
    "        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, queries, keys, values,last_layer = False):\n",
    "        # keys, values: same shape of [N, T_k, C_k]\n",
    "        # queries: A 3d Variable with shape of [N, T_q, C_q]\n",
    "        # Linear projections\n",
    "        Q = self.Q_proj(queries)  # (N, T_q, C)\n",
    "        K = self.K_proj(keys)  # (N, T_q, C)\n",
    "        V = self.V_proj(values)  # (N, T_q, C)\n",
    "        # Split and concat\n",
    "        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        # Multiplication\n",
    "        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n",
    "        # Scale\n",
    "        outputs = outputs / (K_.size()[-1] ** 0.5)\n",
    "\n",
    "        # Activation\n",
    "        if last_layer == False:\n",
    "            outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n",
    "        # Query Masking\n",
    "        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n",
    "        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n",
    "        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n",
    "        outputs = outputs * query_masks\n",
    "        # Dropouts\n",
    "        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n",
    "        if last_layer == True:\n",
    "            return outputs\n",
    "        # Weighted sum\n",
    "        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
