{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bb367-e427-41cf-b4d6-cb789eebbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a48010-202c-4c60-9c8f-9eb45405d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional random field.\n",
    "    This module implements a conditional random field [LMP01]_. The forward computation\n",
    "    of this class computes the log likelihood of the given sequence of tags and\n",
    "    emission score tensor. This class also has `~CRF.decode` method which finds\n",
    "    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n",
    "    Args:\n",
    "        num_tags: Number of tags.\n",
    "        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n",
    "    Attributes:\n",
    "        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n",
    "            ``(num_tags,)``.\n",
    "        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n",
    "            ``(num_tags, num_tags)``.\n",
    "    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n",
    "       \"Conditional random fields: Probabilistic models for segmenting and\n",
    "       labeling sequence data\". *Proc. 18th International Conf. on Machine\n",
    "       Learning*. Morgan Kaufmann. pp. 282â€“289.\n",
    "    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(self, emissions: torch.Tensor,\n",
    "                tags: torch.LongTensor,\n",
    "                mask: Optional[torch.ByteTensor] = None,\n",
    "                reduction: str = 'mean') -> torch.Tensor:\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n",
    "                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "            reduction: Specifies  the reduction to apply to the output:\n",
    "                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n",
    "                ``sum``: the output will be summed over batches. ``mean``: the output will be\n",
    "                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n",
    "        Returns:\n",
    "            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n",
    "            reduction is ``none``, ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8, device=tags.device)\n",
    "        if mask.dtype != torch.uint8:\n",
    "            mask = mask.byte()\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        return llh.sum() / mask.float().sum()\n",
    "    \n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None,\n",
    "               nbest: Optional[int] = None,\n",
    "               pad_tag: Optional[int] = None) -> List[List[List[int]]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "            nbest (`int`): Number of most probable paths for each sequence\n",
    "            pad_tag (`int`): Tag at padded positions. Often input varies in length and\n",
    "                the length will be padded to the maximum length in the batch. Tags at\n",
    "                the padded positions will be assigned with a padding tag, i.e. `pad_tag`\n",
    "        Returns:\n",
    "            A PyTorch tensor of the best tag sequence for each batch of shape\n",
    "            (nbest, batch_size, seq_length)\n",
    "        \"\"\"\n",
    "        if nbest is None:\n",
    "            nbest = 1\n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2], dtype=torch.uint8,\n",
    "                              device=emissions.device)\n",
    "        if mask.dtype != torch.uint8:\n",
    "            mask = mask.byte()\n",
    "        self._validate(emissions, mask=mask)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        if nbest == 1:\n",
    "            return self._viterbi_decode(emissions, mask, pad_tag).unsqueeze(0)\n",
    "        return self._viterbi_decode_nbest(emissions, mask, nbest, pad_tag)\n",
    "\n",
    "    def _validate(self, emissions: torch.Tensor,\n",
    "                  tags: Optional[torch.LongTensor] = None,\n",
    "                  mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "                \n",
    "    def _compute_score(self, emissions: torch.Tensor,\n",
    "                       tags: torch.LongTensor,\n",
    "                       mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(self, emissions: torch.Tensor,\n",
    "                            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor,\n",
    "                        pad_tag: Optional[int] = None) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        # return: (batch_size, seq_length)\n",
    "        if pad_tag is None:\n",
    "            pad_tag = 0\n",
    "\n",
    "        device = emissions.device\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history_idx = torch.zeros((seq_length, batch_size, self.num_tags),\n",
    "                                  dtype=torch.long, device=device)\n",
    "        oor_idx = torch.zeros((batch_size, self.num_tags),\n",
    "                              dtype=torch.long, device=device)\n",
    "        oor_tag = torch.full((seq_length, batch_size), pad_tag,\n",
    "                             dtype=torch.long, device=device)\n",
    "\n",
    "        # - score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        #   value at column j stores the score of the best tag sequence so far that ends\n",
    "        #   with tag j\n",
    "        # - history_idx saves where the best tags candidate transitioned from; this is used\n",
    "        #   when we trace back the best tag sequence\n",
    "        # - oor_idx saves the best tags candidate transitioned from at the positions\n",
    "        #   where mask is 0, i.e. out of range (oor)\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(-1), next_score, score)\n",
    "            indices = torch.where(mask[i].unsqueeze(-1), indices, oor_idx)\n",
    "            history_idx[i - 1] = indices\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        end_score = score + self.end_transitions\n",
    "        _, end_tag = end_score.max(dim=1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "\n",
    "        # insert the best tag at each sequence end (last position with mask == 1)\n",
    "        history_idx = history_idx.transpose(1, 0).contiguous()\n",
    "        history_idx.scatter_(1, seq_ends.view(-1, 1, 1).expand(-1, 1, self.num_tags),\n",
    "                             end_tag.view(-1, 1, 1).expand(-1, 1, self.num_tags))\n",
    "        history_idx = history_idx.transpose(1, 0).contiguous()\n",
    "\n",
    "        # The most probable path for each sequence\n",
    "        best_tags_arr = torch.zeros((seq_length, batch_size),\n",
    "                                    dtype=torch.long, device=device)\n",
    "        best_tags = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "        for idx in range(seq_length - 1, -1, -1):\n",
    "            best_tags = torch.gather(history_idx[idx], 1, best_tags)\n",
    "            best_tags_arr[idx] = best_tags.data.view(batch_size)\n",
    "\n",
    "        return torch.where(mask, best_tags_arr, oor_tag).transpose(0, 1)\n",
    "    \n",
    "    def _viterbi_decode_nbest(self, emissions: torch.FloatTensor,\n",
    "                              mask: torch.ByteTensor,\n",
    "                              nbest: int,\n",
    "                              pad_tag: Optional[int] = None) -> List[List[List[int]]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        # return: (nbest, batch_size, seq_length)\n",
    "        if pad_tag is None:\n",
    "            pad_tag = 0\n",
    "\n",
    "        device = emissions.device\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history_idx = torch.zeros((seq_length, batch_size, self.num_tags, nbest),\n",
    "                                  dtype=torch.long, device=device)\n",
    "        oor_idx = torch.zeros((batch_size, self.num_tags, nbest),\n",
    "                              dtype=torch.long, device=device)\n",
    "        oor_tag = torch.full((seq_length, batch_size, nbest), pad_tag,\n",
    "                             dtype=torch.long, device=device)\n",
    "\n",
    "        # + score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        #   value at column j stores the score of the best tag sequence so far that ends\n",
    "        #   with tag j\n",
    "        # + history_idx saves where the best tags candidate transitioned from; this is used\n",
    "        #   when we trace back the best tag sequence\n",
    "        # - oor_idx saves the best tags candidate transitioned from at the positions\n",
    "        #   where mask is 0, i.e. out of range (oor)\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            if i == 1:\n",
    "                broadcast_score = score.unsqueeze(-1)\n",
    "                broadcast_emission = emissions[i].unsqueeze(1)\n",
    "                # shape: (batch_size, num_tags, num_tags)\n",
    "                next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "            else:\n",
    "                broadcast_score = score.unsqueeze(-1)\n",
    "                broadcast_emission = emissions[i].unsqueeze(1).unsqueeze(2)\n",
    "                # shape: (batch_size, num_tags, nbest, num_tags)\n",
    "                next_score = broadcast_score + self.transitions.unsqueeze(1) + broadcast_emission\n",
    "\n",
    "            # Find the top `nbest` maximum score over all possible current tag\n",
    "            # shape: (batch_size, nbest, num_tags)\n",
    "            next_score, indices = next_score.view(batch_size, -1, self.num_tags).topk(nbest, dim=1)\n",
    "\n",
    "            if i == 1:\n",
    "                score = score.unsqueeze(-1).expand(-1, -1, nbest)\n",
    "                indices = indices * nbest\n",
    "\n",
    "            # convert to shape: (batch_size, num_tags, nbest)\n",
    "            next_score = next_score.transpose(2, 1)\n",
    "            indices = indices.transpose(2, 1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags, nbest)\n",
    "            score = torch.where(mask[i].unsqueeze(-1).unsqueeze(-1), next_score, score)\n",
    "            indices = torch.where(mask[i].unsqueeze(-1).unsqueeze(-1), indices, oor_idx)\n",
    "            history_idx[i - 1] = indices\n",
    "\n",
    "        # End transition score shape: (batch_size, num_tags, nbest)\n",
    "        end_score = score + self.end_transitions.unsqueeze(-1)\n",
    "        _, end_tag = end_score.view(batch_size, -1).topk(nbest, dim=1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "\n",
    "        # insert the best tag at each sequence end (last position with mask == 1)\n",
    "        history_idx = history_idx.transpose(1, 0).contiguous()\n",
    "        history_idx.scatter_(1, seq_ends.view(-1, 1, 1, 1).expand(-1, 1, self.num_tags, nbest),\n",
    "                             end_tag.view(-1, 1, 1, nbest).expand(-1, 1, self.num_tags, nbest))\n",
    "        history_idx = history_idx.transpose(1, 0).contiguous()\n",
    "\n",
    "        # The most probable path for each sequence\n",
    "        best_tags_arr = torch.zeros((seq_length, batch_size, nbest),\n",
    "                                    dtype=torch.long, device=device)\n",
    "        best_tags = torch.arange(nbest, dtype=torch.long, device=device) \\\n",
    "                         .view(1, -1).expand(batch_size, -1)\n",
    "        for idx in range(seq_length - 1, -1, -1):\n",
    "            best_tags = torch.gather(history_idx[idx].view(batch_size, -1), 1, best_tags)\n",
    "            best_tags_arr[idx] = best_tags.data.view(batch_size, -1) // nbest\n",
    "\n",
    "        return torch.where(mask.unsqueeze(-1), best_tags_arr, oor_tag).permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5c5b0-6e7e-4425-a1bc-d55e48099a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = F.dropout(F.relu(self.linear1(x)), p=self.dropout_rate, training=self.training)\n",
    "        x_proj = self.linear2(x_proj)\n",
    "        return x_proj\n",
    "\n",
    "\n",
    "class PoolerStartLogits(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(PoolerStartLogits, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, hidden_states, p_mask=None):\n",
    "        x = self.dense(hidden_states)\n",
    "        return x\n",
    "\n",
    "class PoolerEndLogits(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(PoolerEndLogits, self).__init__()\n",
    "        self.dense_0 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dense_1 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, hidden_states, start_positions=None, p_mask=None):\n",
    "        x = self.dense_0(torch.cat([hidden_states, start_positions], dim=-1))\n",
    "        x = self.activation(x)\n",
    "        x = self.LayerNorm(x)\n",
    "        x = self.dense_1(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
