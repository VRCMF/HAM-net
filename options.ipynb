{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c43358-8742-4ece-81c4-192bcaff7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f7ee7-9a9b-4cb3-8a24-827437d12487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_parser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', default='../data/weak_data/rtg_sad', help=\"kir_oper ; lah_lah ; rtg_sad\")\n",
    "  parser.add_argument('--model_dir', default='../model/linear_no_pretrain_lsr_rtg_sad', help=\"Directory containing the dataset\")\n",
    "  parser.add_argument('--pretrained_model', default='../tools/bert-base-finnish-uncased-v1', help=\"Directory containing the BERT model in PyTorch\")\n",
    "  parser.add_argument('--bert_model_dir', default='../tools/bert-base-finnish-uncased-v1', help=\"Directory containing the BERT model in PyTorch\")\n",
    "  parser.add_argument(\"--pretrain\", action=\"store_true\", default=False, help=\"full finetuning\")\n",
    "  parser.add_argument('--pretrain_path', default='../save_models/pretrain_weight/checkpoint-100000', help=\"Directory containing the BERT model in PyTorch\")\n",
    "  #parser.add_argument('--model_dir', default='experiments/base_model', help=\"Directory containing params.json\")\n",
    "  parser.add_argument('--seed', type=int, default=2021, help=\"random seed for initialization\")\n",
    "  parser.add_argument(\"--batch_size\", type=int, default=1, help=\"The batch size of model training\")\n",
    "  parser.add_argument(\"--max_len\", type=int, default=512, help=\"Max length of sentences\")\n",
    "  parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "  parser.add_argument(\"--drop\", type=float, default=0.3)\n",
    "  parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "  parser.add_argument(\"--epoch_record\", type=int, default=1)\n",
    "  parser.add_argument(\"--weight_tuning\", action=\"store_const\", const=True, default=False)\n",
    "  parser.add_argument(\"--n_epochs\", type=int, default=10, help=\"Training epochs\")\n",
    "  parser.add_argument('--gpus', default='0', help='use comma for multiple gpus')\n",
    "  parser.add_argument('--loss_type', default='lsr', help='ce, lsr')\n",
    "  parser.add_argument('--decoder', default='linear', help='linear, crf')\n",
    "  parser.add_argument(\"--noise_loss\", action=\"store_true\", default=True, help=\"apply noise-aware loss\")\n",
    "  parser.add_argument(\"--BERT_MAX_LENGTH\", type=int, default=512, help=\"Maximum sequence length for BERT, last tokens selected after MAX_LENGTH has been applied. Bio_ClinicalBert has max lenght 512.\")\n",
    "  parser.add_argument(\"--full_finetuning\", action=\"store_true\", default=True, help=\"full finetuning\")\n",
    "  parser.add_argument('--restore_file', default=None,\n",
    "                    help=\"Optional, name of the file in --model_dir containing weights to reload before training\")\n",
    "  parser.add_argument(\"--patience_num\", type=int, default=5, help=\"early stopping patience rounds\")\n",
    "  parser.add_argument(\"--patience\", type=float, default=0.02, help=\"early stopping patience rounds\")\n",
    "  parser.add_argument('--fp16', default=False, action='store_true', help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "  \n",
    "  #args = parser.parse_args()\n",
    "  args, unknown = parser.parse_known_args()\n",
    "  command = ' '.join(['python'] + sys.argv)\n",
    "  args.command = command\n",
    "\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d784a-bd92-4d3b-ba6c-993de4575b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Databricks notebook source\n",
    "# import argparse\n",
    "# import sys\n",
    "\n",
    "# def args_parser():\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument('--data_dir', default='../data/weak_data/KIR', help=\"Directory containing the dataset\")\n",
    "#   parser.add_argument('--model_dir', default='../model/linear_pretrain', help=\"Directory containing the dataset\")\n",
    "#   parser.add_argument('--pretrained_model', default='../tools/bert-base-finnish-uncased-v1', help=\"Directory containing the BERT model in PyTorch\")\n",
    "#   parser.add_argument('--bert_model_dir', default='../tools/bert-base-finnish-uncased-v1', help=\"Directory containing the BERT model in PyTorch\")\n",
    "#   parser.add_argument(\"--pretrain\", action=\"store_true\", default=False, help=\"full finetuning\")\n",
    "#   parser.add_argument('--model', default='rnn', help='bert, rnn, cnn')\n",
    "#   parser.add_argument('--pretrain_path', default='../save_models/pretrain_weight/checkpoint-100000', help=\"Directory containing the BERT model in PyTorch\")\n",
    "#   #parser.add_argument('--model_dir', default='experiments/base_model', help=\"Directory containing params.json\")\n",
    "#   parser.add_argument('--seed', type=int, default=2021, help=\"random seed for initialization\")\n",
    "#   parser.add_argument(\"--batch_size\", type=int, default=8, help=\"The batch size of model training\")\n",
    "#   parser.add_argument(\"--max_len\", type=int, default=512, help=\"Max length of sentences\")\n",
    "#   parser.add_argument(\"--lr\", type=float, default=1e-6)\n",
    "#   parser.add_argument(\"--drop\", type=float, default=0.1)\n",
    "#   parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "#   parser.add_argument(\"--epoch_record\", type=int, default=1)\n",
    "#   parser.add_argument(\"--weight_tuning\", action=\"store_const\", const=True, default=False)\n",
    "#   parser.add_argument(\"--n_epochs\", type=int, default=15, help=\"Training epochs\")\n",
    "#   parser.add_argument('--gpus', default='0', help='use comma for multiple gpus')\n",
    "#   parser.add_argument('--loss_type', default='ce', help='fl, ce, lsr')\n",
    "#   parser.add_argument('--decoder', default='linear', help='linear, crf, lan')\n",
    "#   parser.add_argument(\"--noise_loss\", action=\"store_true\", default=True, help=\"apply noise-aware loss\")\n",
    "#   parser.add_argument(\"--BERT_MAX_LENGTH\", type=int, default=512, help=\"Maximum sequence length for BERT, last tokens selected after MAX_LENGTH has been applied. Bio_ClinicalBert has max lenght 512.\")\n",
    "#   parser.add_argument(\"--full_finetuning\", action=\"store_true\", default=True, help=\"full finetuning\")\n",
    "#   parser.add_argument('--restore_file', default=None,\n",
    "#                     help=\"Optional, name of the file in --model_dir containing weights to reload before training\")\n",
    "#   parser.add_argument(\"--patience_num\", type=int, default=10, help=\"early stopping patience rounds\")\n",
    "#   parser.add_argument(\"--patience\", type=float, default=0.02, help=\"early stopping patience rounds\")\n",
    "#   parser.add_argument('--fp16', default=False, action='store_true', help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "  \n",
    "#   #args = parser.parse_args()\n",
    "#   args, unknown = parser.parse_known_args()\n",
    "#   command = ' '.join(['python'] + sys.argv)\n",
    "#   args.command = command\n",
    "\n",
    "#   return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b634ff-c2e2-45ba-8091-12ae66c4be5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
