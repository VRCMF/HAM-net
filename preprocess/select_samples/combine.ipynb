{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda2971-b3fc-4d59-8f22-3aabc499c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import davies_bouldin_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import davies_bouldin_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99084cba-92c4-4b2c-a1b4-8abe471b0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sampling(kmeans, label, reduced_fea, cluster_idx, num_samples):\n",
    "    incluster_dist_list = []\n",
    "    cluster_center = kmeans.cluster_centers_[label]\n",
    "    for idx in cluster_idx:\n",
    "        point = reduced_fea[idx]\n",
    "        dist = euclidean(point, cluster_center)\n",
    "        incluster_dist_list.append(dist)\n",
    "    normalizd_dist = np.array(incluster_dist_list)\n",
    "    inverse_norm_dist = 1/normalizd_dist\n",
    "    sampled_idx = np.random.choice(cluster_idx, num_samples, p=inverse_norm_dist/inverse_norm_dist.sum()) \n",
    "    \n",
    "    return sampled_idx, inverse_norm_dist/inverse_norm_dist.sum()\n",
    "\n",
    "def max_min_scaling(inputs):\n",
    "    return (inputs  - min(inputs)) /(max(inputs) - min(inputs))\n",
    "\n",
    "def in_cluster_sample(sent_list, sets, kmeans, kmean_cluster, sample_per_cluster, reduced_fea):\n",
    "    sample_str = ''\n",
    "    p_str = ''\n",
    "    cluster_idx_str = ''\n",
    "    sample_list = []\n",
    "    prob_list = []\n",
    "    annotate_sent_list = []\n",
    "    un_annotate_sent_list = []\n",
    "    cluster_idx_list = []\n",
    "    \n",
    "    for i in range(kmean_cluster):\n",
    "        cluster_idx = np.where(kmeans.labels_ == i)[0].tolist() \n",
    "        # cluster_idx = np.where(kmeans.labels_ == 0)[0].tolist() \n",
    "        # sample = np.random.choice(cluster_idx, sample_per_cluster) \n",
    "        sample, p = cluster_sampling(kmeans, i, reduced_fea, cluster_idx, sample_per_cluster)\n",
    "        diff_idx = list(set(cluster_idx) - set(sample))\n",
    "        annotate_sent = []\n",
    "        un_annotate_sent = []\n",
    "        for j in sample:\n",
    "            annotate_sent.append(sent_list[j])\n",
    "            \n",
    "        for z in diff_idx:\n",
    "            un_annotate_sent.append(sent_list[z])\n",
    "            \n",
    "        sample_list += list(sample)\n",
    "        prob_list.append(';'.join([str(item) for item in list(p)]))\n",
    "        annotate_sent_list += list(annotate_sent) \n",
    "        un_annotate_sent_list += list(un_annotate_sent) \n",
    "        cluster_idx_list.append(';'.join([str(item) for item in cluster_idx]))\n",
    "        \n",
    "    df = pd.DataFrame({'index': sample_list, 'sent': annotate_sent_list})\n",
    "    df.to_csv('../../data/weak_data/{}/annotation.csv'.format(sets), index=False)\n",
    "    \n",
    "    df = pd.DataFrame({'index': cluster_idx_list, 'prob': prob_list})\n",
    "    df.to_csv('../../data/weak_data/{}/annotation_prob.csv'.format(sets), index=False)\n",
    "    \n",
    "    with open('../../data/weak_data/{}/annotation.txt'.format(sets), 'w') as f:\n",
    "        f.write('\\n'.join(annotate_sent_list))\n",
    "        f.close()\n",
    "        \n",
    "    with open('../../data/weak_data/{}/un_annotation.txt'.format(sets), 'w') as f:\n",
    "        f.write('\\n'.join(un_annotate_sent_list))\n",
    "        f.close()\n",
    "        \n",
    "def gen_sent_list(sets, pca_dim, kmean_cluster, sample_per_cluster):\n",
    "    input_text_path = '../../data/weak_data/{}/full_text.txt'.format(sets)\n",
    "    model = SentenceTransformer('../../tools/sbert-uncased-finnish-paraphrase')\n",
    "    sent_list = []\n",
    "    with open(input_text_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            sent_list.append(line.strip())\n",
    "        f.close()\n",
    "    sent_list = list(set(sent_list))\n",
    "    # tfidf_vect = TfidfVectorizer()\n",
    "    # tfidf = tfidf_vect.fit_transform(sent_list)\n",
    "    sent_vec = model.encode(sent_list)\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    reduced_fea = pca.fit_transform(sent_vec)\n",
    "    kmeans = KMeans(n_clusters=kmean_cluster).fit(reduced_fea)\n",
    "    \n",
    "    in_cluster_sample(sent_list, sets, kmeans, kmean_cluster, sample_per_cluster, reduced_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d224a3-765a-4751-8e3a-74bd5ef09cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS divergence\n",
    "def KL_div(p_probs, q_probs):    \n",
    "    KL_div = p_probs * np.log(p_probs / q_probs)\n",
    "    return np.sum(KL_div)\n",
    "\n",
    "def JS_Div(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    # normalize\n",
    "    p = p/p.sum()\n",
    "    q = q/q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (KL_div(p, m) + KL_div(q, m)) / 2\n",
    "\n",
    "def num_label_random(sets):\n",
    "    df = pd.read_csv('../../data/weak_data/{}/annotation_random_label.csv'.format(sets))\n",
    "    label_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        label_list += row['label'].split(' ')\n",
    "    count_label = dict(Counter(label_list))\n",
    "    return count_label\n",
    "\n",
    "def num_label_cluster(sets):\n",
    "    df = pd.read_csv('../../data/weak_data/{}/Excel_anno.csv'.format(sets))\n",
    "    label_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if type(row['Label']) == type('a'):\n",
    "            label_list.append(row['Label'])#.split(' ')\n",
    "    count_label = dict(Counter(label_list))\n",
    "    return count_label\n",
    "\n",
    "def num_label_full(sets):\n",
    "    label_list = []\n",
    "    with open('../../data/weak_data/{}/label.txt'.format(sets), 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            label_list += line.strip().split(' ')\n",
    "        f.close()\n",
    "    count_label = dict(Counter(label_list))\n",
    "    return count_label\n",
    "\n",
    "def retrieve_val(label_list, counter):\n",
    "    num_list = []\n",
    "    for i in label_list:\n",
    "        if i in counter.keys():\n",
    "            num_list.append(counter[i])\n",
    "        else:\n",
    "            num_list.append(1e-11)\n",
    "            \n",
    "    return num_list\n",
    "\n",
    "def gen_random_cluster_full_list(sets):\n",
    "    random_counter = num_label_random(sets)\n",
    "    cluster_counter = num_label_cluster(sets)\n",
    "    full_counter = num_label_full(sets)\n",
    "    \n",
    "    full_med_label = list(full_counter.keys())\n",
    "    \n",
    "    rdm = retrieve_val(full_med_label, random_counter)\n",
    "    clr = retrieve_val(full_med_label, cluster_counter)\n",
    "    ful = retrieve_val(full_med_label, full_counter)\n",
    "    \n",
    "    return rdm, clr, ful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f839c1-a221-4285-ae44-4ca5989f7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sent_list_r(sets, num_samples):\n",
    "    input_text_path = '../../data/weak_data/{}/full_text.txt'.format(sets)\n",
    "    input_label_path = '../../data/weak_data/{}/label.txt'.format(sets)\n",
    "    #model = SentenceTransformer('../../tools/sbert-uncased-finnish-paraphrase')\n",
    "    sent_list = []\n",
    "    with open(input_text_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            sent_list.append(line.strip())\n",
    "        f.close()\n",
    "    #sent_list = list(set(sent_list))\n",
    "    \n",
    "    label_list = []\n",
    "    with open(input_label_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            label_list.append(line.strip())\n",
    "        f.close()\n",
    "    \n",
    "    assert len(label_list) == len(sent_list)\n",
    "    \n",
    "    comb_idx_sent_list = []\n",
    "    for i in range(len(sent_list)):\n",
    "        item = (i, sent_list[i], label_list[i])\n",
    "        comb_idx_sent_list.append(item)\n",
    "    \n",
    "    pos_list =  np.random.choice(list(range(len(comb_idx_sent_list))), num_samples)\n",
    "    \n",
    "    random_select_idx_sent_label = [comb_idx_sent_list[i] for i in pos_list]#np.random.choice(comb_idx_sent_list, num_samples)\n",
    "    \n",
    "    select_idx = [item[0] for item in random_select_idx_sent_label]\n",
    "    \n",
    "    select_sent = [item[1] for item in random_select_idx_sent_label]\n",
    "    \n",
    "    select_label = [item[2] for item in random_select_idx_sent_label]\n",
    "    \n",
    "    df = pd.DataFrame({'index': select_idx, 'sent': select_sent})\n",
    "    df.to_csv('../../data/weak_data/{}/annotation_random_sent.csv'.format(sets), index=False)\n",
    "    \n",
    "    df = pd.DataFrame({'index': select_idx, 'label': select_label})\n",
    "    df.to_csv('../../data/weak_data/{}/annotation_random_label.csv'.format(sets), index=False)\n",
    "    \n",
    "    #selected_label\n",
    "    \n",
    "    \n",
    "    #return random_select_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfa5cd-62f8-4052-8824-9a9542af93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sets = 'KIR'\n",
    "    pca_dim = 2\n",
    "    kmean_cluster = 10\n",
    "    sample_per_cluster = 22\n",
    "    gen_sent_list(sets, pca_dim, kmean_cluster, sample_per_cluster)\n",
    "\n",
    "    # KIR, LAH, OPER, RTG, SAD\n",
    "    # 225., 200., 198., 162., 200.\n",
    "    random = 0\n",
    "    cluster = 0\n",
    "    for i in range(10):\n",
    "        #sets = 'KIR'\n",
    "        num_samples = 225\n",
    "        gen_sent_list_r(sets, num_samples)\n",
    "        res = gen_random_cluster_full_list(sets)\n",
    "        result_random_full= JS_Div(res[0][1:], res[2][1:])\n",
    "        result_cluster_full= JS_Div(res[1][1:], res[2][1:])\n",
    "        random += result_random_full\n",
    "        cluster += result_cluster_full\n",
    "    if random > cluster:\n",
    "        print('{}: cluster better'.format(sets))\n",
    "        print(random)\n",
    "        print(cluster)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c9131-f660-487c-8631-f528d38c456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307888ee-f47a-477f-850c-c93f1d7e3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sets = 'OPER'\n",
    "    pca_dim = 2\n",
    "    kmean_cluster = 10\n",
    "    sample_per_cluster = 20\n",
    "    gen_sent_list(sets, pca_dim, kmean_cluster, sample_per_cluster)\n",
    "\n",
    "    # KIR, LAH, OPER, RTG, SAD\n",
    "    # 225., 200., 198., 162., 200.\n",
    "    random = 0\n",
    "    cluster = 0\n",
    "    for i in range(10):\n",
    "        #sets = 'KIR'\n",
    "        num_samples = 203\n",
    "        gen_sent_list_r(sets, num_samples)\n",
    "        res = gen_random_cluster_full_list(sets)\n",
    "        result_random_full= JS_Div(res[0][1:], res[2][1:])\n",
    "        result_cluster_full= JS_Div(res[1][1:], res[2][1:])\n",
    "        random += result_random_full\n",
    "        cluster += result_cluster_full\n",
    "    if random > cluster:\n",
    "        print('{}: cluster better'.format(sets))\n",
    "        print(random)\n",
    "        print(cluster)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7e5a8-fefb-488a-8367-e6f0683dbd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sets = 'SAD'\n",
    "    pca_dim = 2\n",
    "    kmean_cluster = 10\n",
    "    sample_per_cluster = 20\n",
    "    gen_sent_list(sets, pca_dim, kmean_cluster, sample_per_cluster)\n",
    "\n",
    "    # KIR, LAH, OPER, RTG, SAD\n",
    "    # 225., 200., 198., 162., 200.\n",
    "    random = 0\n",
    "    cluster = 0\n",
    "    for i in range(10):\n",
    "        #sets = 'KIR'\n",
    "        num_samples = 204\n",
    "        gen_sent_list_r(sets, num_samples)\n",
    "        res = gen_random_cluster_full_list(sets)\n",
    "        result_random_full= JS_Div(res[0][1:], res[2][1:])\n",
    "        result_cluster_full= JS_Div(res[1][1:], res[2][1:])\n",
    "        random += result_random_full\n",
    "        cluster += result_cluster_full\n",
    "    if random > cluster:\n",
    "        print('{}: cluster better'.format(sets))\n",
    "        print(random)\n",
    "        print(cluster)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b404829-50e2-4f94-ac19-af566fea89ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
