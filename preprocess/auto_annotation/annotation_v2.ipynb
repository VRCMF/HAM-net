{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87b9e0-d368-402d-b55b-efaa2c22986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c5bb7-be2a-4b00-a707-a2b1d82afc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = '../data/weak_data/stop_words.txt'\n",
    "stop_list = []\n",
    "with open(stop_words_path, 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        stop_list.append(i.strip()) \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b1ce7-4430-4613-b0e0-be1e42e42d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dict(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         dicts = json.load(f).decode('utf8')\n",
    "#         f.close()\n",
    "#     return dicts\n",
    "\n",
    "# def Mesh_dict(mesh_path):\n",
    "#   mesh_dict = {}\n",
    "#   with open(mesh_path, encoding='utf-8') as file1:\n",
    "#     mem_string = \"\"\n",
    "#     for line in file1:\n",
    "#       if line == '\\n':\n",
    "#         if mem_string.startswith('mesh:'):\n",
    "#           med_list = mem_string.split(';')\n",
    "#           try:\n",
    "#             key = re.findall(r'mesh:[a-z]*[A-Z][0-9]+', med_list[0])[0]\n",
    "#           except IndexError:\n",
    "#             pass\n",
    "#           mesh_dict[key] = {}\n",
    "#           for i in med_list:\n",
    "#             item = i.strip()\n",
    "#             if item.startswith('skos:broader'):\n",
    "#               mesh_dict[key]['broader'] = list()\n",
    "#               mesh_dict[key]['broader'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "#             if item.startswith('skos:prefLabel'):\n",
    "#               # add other language\n",
    "#               mesh_dict[key]['AllLabel'] = list()\n",
    "#               if '\"@fi' in item:\n",
    "#                 mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@fi', item)[0][1:-4])\n",
    "#               if '\"@en' in item:\n",
    "#                 mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@en', item)[0][1:-4])\n",
    "#               if '\"@sv' in item:\n",
    "#                 mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@sv', item)[0][1:-4])\n",
    "#               try:\n",
    "#                 mesh_dict[key]['prefLabel'] = re.findall(r'\".*\"@fi', item)[0][1:-4]\n",
    "#               except IndexError:\n",
    "#                 mesh_dict[key]['prefLabel'] = re.findall(r'\".*\"@en', item)[0][1:-4]\n",
    "#             if item.startswith('skos:altLabel'):\n",
    "#               mesh_dict[key]['altLabel'] = list()\n",
    "#               alt_list = re.findall(r'\".*\"@fi', item)\n",
    "#               for i in alt_list:\n",
    "#                 mesh_dict[key]['altLabel'].append(i[1:-4])\n",
    "#             if item.startswith('skos:narrower'):\n",
    "#               mesh_dict[key]['narrower'] = list()\n",
    "#               mesh_dict[key]['narrower'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "#             if item.startswith('skos:related'):\n",
    "#               mesh_dict[key]['related'] = list()\n",
    "#               mesh_dict[key]['related'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "#         mem_string = \"\"\n",
    "#         key = \"\"\n",
    "#       else:\n",
    "#         mem_string = mem_string + line\n",
    "        \n",
    "#   return mesh_dict\n",
    "\n",
    "# def Estab_rel(sum_dict):\n",
    "#   rel_list = []\n",
    "#   for k, v in sum_dict.items():\n",
    "#     if 'broader' in list(v.keys()):\n",
    "#       for b in v['broader']:\n",
    "#         rel_list.append((b, k))\n",
    "#     if 'narrower' in list(v.keys()):\n",
    "#       for n in v['narrower']:\n",
    "#         rel_list.append((k, n))\n",
    "          \n",
    "#   return rel_list\n",
    "\n",
    "# def traverse(hierarchy, graph, names):\n",
    "#     for name in names:\n",
    "#         hierarchy[name] = traverse({}, graph, graph[name])\n",
    "#     return hierarchy\n",
    "\n",
    "# def Build_graph(rel_list):\n",
    "#   graph = {name: set() for tup in rel_list for name in tup}\n",
    "#   has_parent = {name: False for tup in rel_list for name in tup}\n",
    "#   for parent, child in rel_list:\n",
    "#       graph[parent].add(child)\n",
    "#       has_parent[child] = True\n",
    "\n",
    "#   roots = [name for name, parents in has_parent.items() if not parents]\n",
    "  \n",
    "#   res_dict = traverse({}, graph, roots)\n",
    "  \n",
    "#   return res_dict, roots\n",
    "\n",
    "# def flatten(d):    \n",
    "#   res = []  # Result list\n",
    "#   for key, val in d.items():\n",
    "#     res.append(key)\n",
    "#     res += flatten(val)\n",
    "#     if d == {}:\n",
    "#       res = d\n",
    "#   return res\n",
    "\n",
    "# def Flat_dict(res_dict):\n",
    "#   flat_dict = {}\n",
    "#   for k, v in res_dict.items():\n",
    "#     flat_dict[k] = flatten(v)\n",
    "#   return flat_dict\n",
    "\n",
    "# def Retrieve_tree(label, f_dicts, m_dicts, r_nodes):\n",
    "#   label4code = [i[0] for i in r_nodes if i[1] == label][0]\n",
    "#   item_list = f_dicts[label4code]\n",
    "#   label_list = [m_dicts[j]['prefLabel'] for j in item_list]\n",
    "#   return label_list\n",
    "\n",
    "# # compressed dictionary\n",
    "# def create_new_dicts(flat_dict, mesh_dict):\n",
    "#   label2code = {}\n",
    "#   for k, v in mesh_dict.items():\n",
    "#     if k != '':\n",
    "#       label2code[v['prefLabel']] = k\n",
    "#   # comp_dicts = {\n",
    "#   #   'Anatomical-structure': ['tuki- ja liikuntaelimistö', 'kudokset', 'hermosto', 'purentaelimistö', 'veri- ja immuunijärjestelmät', 'sydän ja verenkiertoelimistö', 'ruuansulatuselimistö', 'hengityselimet', 'umpieritysjärjestelmä', 'aistinelimet', 'virtsa- ja sukupuolielimet', 'kehonalueet', 'iho ja ihon apuelimet'],\n",
    "#   #   'Body-function-and-measurement': ['lisääntymis- ja virtsaelinten fysiologiset ilmiöt', 'muskuloskeletaaliset ja hermostolliset fysiologiset ilmiöt', 'ruuansulatuskanavan ja suun fysiologiset ilmiöt', 'ihovaipan fysiologiset ilmiöt', 'fysiologiset ilmiöt', 'verenkierto- ja hengitysfysiologia', 'aineenvaihdunta'],\n",
    "#   #   'Medical-condition': ['purentaelimistön sairaudet', 'patologiset tilat, löydökset ja oireet', 'hermoston sairaudet', 'kasvaimet', 'ravitsemus- ja aineenvaihduntasairaudet', 'korva-, nenä- ja kurkkutaudit', 'miesten virtsa- ja sukupuolielinten sairaudet', 'iho- ja sidekudossairaudet', 'tuki- ja liikuntaelinten sairaudet', 'immuunijärjestelmän sairaudet', 'endokriiniset sairaudet', 'hengityselinten taudit', 'veri- ja lymfaattiset taudit', 'naisen virtsa- ja sukupuolielinten sairaudet ja raskauskomplikaatiot', 'synnynnäiset, perinnölliset ja vastasyntyneen taudit ja epämuodostumat', 'sydän- ja verisuonitaudit', 'infektiot', 'silmätaudit', 'haavat ja vammat'],\n",
    "#   #   'Medical-device': ['tarvikkeet ja varusteet'],\n",
    "#   #   'Medical-procedure': ['leikkaukset', 'diagnoosi', 'hoitomenetelmät', 'informaatiotiede', 'anestesia ja analgesia', 'tutkimusmenetelmät'],\n",
    "#   #   'Medication': ['lääkevalmisteet', 'polysykliset yhdisteet', 'kemikaalien vaikutukset ja käyttötarkoitukset', 'heterosykliset yhdisteet', 'epäorgaaniset kemikaalit', 'orgaaniset kemikaalit']\n",
    "#   # }\n",
    "#     comp_dicts = {\n",
    "#     'Anatomical-structure': ['tuki- ja liikuntaelimistö', 'kudokset', 'hermosto', 'purentaelimistö', 'veri- ja immuunijärjestelmät', 'sydän ja verenkiertoelimistö', 'ruuansulatuselimistö', 'hengityselimet', 'umpieritysjärjestelmä', 'aistinelimet', 'virtsa- ja sukupuolielimet', 'kehonalueet', 'iho ja ihon apuelimet'],\n",
    "#     'Body-function-and-measurement': ['lisääntymis- ja virtsaelinten fysiologiset ilmiöt', 'muskuloskeletaaliset ja hermostolliset fysiologiset ilmiöt', 'ruuansulatuskanavan ja suun fysiologiset ilmiöt', 'ihovaipan fysiologiset ilmiöt', 'fysiologiset ilmiöt', 'verenkierto- ja hengitysfysiologia', 'aineenvaihdunta'],\n",
    "#     'Medical-condition': ['purentaelimistön sairaudet', 'patologiset tilat, löydökset ja oireet', 'hermoston sairaudet', 'kasvaimet', 'ravitsemus- ja aineenvaihduntasairaudet', 'korva-, nenä- ja kurkkutaudit', 'virtsa- ja sukupuolielinten sairaudet', 'iho- ja sidekudossairaudet', 'tuki- ja liikuntaelinten sairaudet', 'immuunijärjestelmän sairaudet', 'endokriiniset sairaudet', 'hengityselinten taudit', 'veri- ja lymfaattiset taudit', 'synnynnäiset, perinnölliset ja vastasyntyneen taudit ja epämuodostumat', 'sydän- ja verisuonitaudit', 'infektiot', 'silmätaudit', 'haavat ja vammat'],\n",
    "#     'Medical-device': ['tarvikkeet ja varusteet'],\n",
    "#     'Medical-procedure': ['leikkaukset', 'diagnoosi', 'hoitomenetelmät', 'informaatiotiede', 'anestesia ja analgesia', 'tutkimusmenetelmät'],\n",
    "#     'Medication': ['lääkevalmisteet', 'polysykliset yhdisteet', 'kemikaalien vaikutukset ja käyttötarkoitukset', 'heterosykliset yhdisteet', 'epäorgaaniset kemikaalit', 'orgaaniset kemikaalit']\n",
    "#   }\n",
    "#   # then check each element in the medical dictionary\n",
    "#   sub_item_collect = []\n",
    "#   code_dicts = {}\n",
    "#   for k, v in comp_dicts.items():\n",
    "#     code_dicts[k] = []\n",
    "#     for item in v:\n",
    "#       code_dicts[k].append(label2code[item])\n",
    "#     # append separate value list\n",
    "#     sub_item_collect += v\n",
    "#   # size of updated medical dictionary\n",
    "#   size_dicts = len(sub_item_collect)\n",
    "#   # use flatten dictionary to construct new dictionary\n",
    "#   new_dicts = {}\n",
    "#   for k, v in code_dicts.items():\n",
    "#     new_dicts[k] = []\n",
    "#     for item in v:\n",
    "#       elements = flat_dict[item]\n",
    "#       new_dicts[k] += elements\n",
    "#       new_dicts[k] += [item]\n",
    "\n",
    "#   return new_dicts\n",
    "\n",
    "# def label_pooling(string):\n",
    "#   str_set = []\n",
    "#   str_element = string.split(';')\n",
    "#   for i in str_element:\n",
    "#     str_set.append(i.lower())\n",
    "  \n",
    "#   return str_set\n",
    "\n",
    "# def token_mapping(source_tok, sub_dict):\n",
    "#   matched_num = 0\n",
    "#   matched_tok = []\n",
    "\n",
    "#   if ' ' in source_tok:\n",
    "#     source = source_tok.split(' ')\n",
    "#   else:\n",
    "#     source = [source_tok]\n",
    "#   source_len = np.sum(np.array([len(i) for i in source]))\n",
    "#   for item in sub_dict:\n",
    "#     if ' ' in item:\n",
    "#       piece = item.split(' ')\n",
    "#     else:\n",
    "#       piece = [item]\n",
    "#     # num of terms is equal\n",
    "#     if len(piece) == len(source):\n",
    "#       matched_count = 0\n",
    "#       for p in range(len(piece)):\n",
    "#         # len of each piece >= len of source piece\n",
    "#         if len(piece[p]) >= len(source[p]):\n",
    "#           for t in range(len(source[p])):\n",
    "#             if piece[p][t] == source[p][t]:\n",
    "#               matched_count += 1\n",
    "#             else:\n",
    "#               break\n",
    "#           if matched_count == source_len:\n",
    "#             matched_num += 1\n",
    "#             matched_tok.append(item)\n",
    "#   return matched_num, matched_tok\n",
    "\n",
    "# def candidates(source, dicts):\n",
    "#   can_len = []\n",
    "#   can = []\n",
    "#   for k, v in dicts.items():\n",
    "#     num, tok = token_mapping(source, v)\n",
    "#     can_len.append(num)\n",
    "#     can.append(tok)\n",
    "#   return can_len, can\n",
    "\n",
    "# def cal_score(source, cans):\n",
    "#   scores = []\n",
    "#   for cat in cans:\n",
    "#     score = []\n",
    "#     for item in cat:\n",
    "#       score.append(fuzz.ratio(source, item))\n",
    "#     if len(score) == 0:\n",
    "#       scores.append(0)\n",
    "#     else:\n",
    "#       scores.append(np.max(np.array(score)))\n",
    "#   return scores\n",
    "\n",
    "# def label_assign(s1, s2):\n",
    "#   s = []\n",
    "#   for i in range(len(s1)):\n",
    "#     if s1[i] > s2[i]:\n",
    "#       s.append(s1[i])  \n",
    "#     else:\n",
    "#       s.append(s2[i])  \n",
    "#   s = np.array(s)\n",
    "#   if np.max(s) == 0:\n",
    "#     max_idx = -1\n",
    "#     max_value = 0\n",
    "#   else:\n",
    "#     max_value = 0\n",
    "#     max_idx = 0\n",
    "#     for z in range(len(s)):\n",
    "#       if s[z] > max_value:\n",
    "#         max_idx = z\n",
    "#         max_value = s[z]\n",
    "#       # if the values are equal, then comparison on two list\n",
    "#       elif s[z] == max_value:\n",
    "#         s1_z = s1[z]\n",
    "#         s1_m = s1[max_idx]\n",
    "#         s2_z = s2[z]\n",
    "#         s2_m = s2[max_idx]\n",
    "#         # one of list, the z win\n",
    "#         if s1_z > s1_m and s2_z == s2_m:\n",
    "#           max_idx = z\n",
    "#         elif s1_z == s1_m and s2_z > s2_m:\n",
    "#           max_idx = z\n",
    "#         else:\n",
    "#           pass\n",
    "\n",
    "#   return max_idx, max_value\n",
    "\n",
    "# def create_res_dicts(a_dict, m_dict):\n",
    "#   res_dicts = {}\n",
    "#   res_dicts_alt = {}\n",
    "#   for k, v in a_dict.items():\n",
    "#     md_items = v\n",
    "#     label_lake = []\n",
    "#     label_lake_alt = []\n",
    "#     for i in md_items:\n",
    "#         cur_item = m_dict[i]\n",
    "#         if 'altLabel' in cur_item.keys():\n",
    "#             cur_alt = cur_item['altLabel']\n",
    "#         else:\n",
    "#             cur_alt = []\n",
    "#         cur_pref = cur_item['prefLabel']\n",
    "#         cur_all = cur_item['AllLabel']\n",
    "#         if len(cur_alt) > 0:\n",
    "#             label_lake_alt += cur_alt\n",
    "#         if cur_pref != '':\n",
    "#           label_lake.append(cur_pref)\n",
    "#         if len(cur_all) > 0:\n",
    "#           label_lake += cur_all\n",
    "#     label_lake = list(set(label_lake))\n",
    "#     main_match_string = ';'.join(label_lake)\n",
    "#     res_dicts[k] = label_pooling(main_match_string)\n",
    "\n",
    "#     label_lake_alt = list(set(label_lake_alt))\n",
    "#     main_match_string_alt = ';'.join(label_lake_alt)\n",
    "#     res_dicts_alt[k] = label_pooling(main_match_string_alt)\n",
    "\n",
    "#   return res_dicts, res_dicts_alt\n",
    "\n",
    "# def beam_mapping(source, r_dicts, r_dicts_alt, window, mask_list):\n",
    "  \n",
    "#   labels = []\n",
    "#   label_space =  list(r_dicts.keys())\n",
    "#   source_ele = source#.split(' ')\n",
    "#   source_len = len(source_ele)\n",
    "#   idx = 0\n",
    "  \n",
    "#   while idx < source_len:\n",
    "#     if mask_list[idx] == 'T':\n",
    "#       if (source_len - idx) >= window:\n",
    "#         step = window\n",
    "#       else:\n",
    "#         step = source_len - idx\n",
    "#       candidate_idx = []\n",
    "#       candidate_value = []\n",
    "#       for shift in range(1, step+1):\n",
    "#         term = source_ele[idx:idx + shift]\n",
    "#         term_str = ' '.join(term)\n",
    "#         _, candidate = candidates(term_str, r_dicts)\n",
    "#         _, candidate_alt = candidates(term_str, r_dicts_alt)\n",
    "#         s1 = cal_score(term_str, candidate)\n",
    "#         s2 = cal_score(term_str, candidate_alt)\n",
    "#         max_idx, max_value = label_assign(s1, s2)\n",
    "#         candidate_idx.append(max_idx)\n",
    "#         candidate_value.append(max_value)\n",
    "#       if np.max(np.array(candidate_value)) > 0:\n",
    "#         # the position of best matching\n",
    "#         max_pos = np.argmax(np.array(candidate_value))\n",
    "#         # the label index of best matching\n",
    "#         label_idx = candidate_idx[max_pos]\n",
    "#         win_shift = max_pos + 1\n",
    "#         idx += win_shift\n",
    "#         label = label_space[label_idx]\n",
    "#         if win_shift == 1:\n",
    "#           labels.append('B-' + label)\n",
    "#         else:\n",
    "#           labels.append('B-' + label)\n",
    "#           for i in range(win_shift - 1):\n",
    "#             labels.append('I-' + label)\n",
    "#       else:\n",
    "#         labels.append('O')\n",
    "#         idx += 1\n",
    "#     else:\n",
    "#       labels.append('O')\n",
    "#       idx += 1\n",
    "\n",
    "#   return labels\n",
    "\n",
    "# def lemma_tokens(input_path):\n",
    "#   with open(input_path, 'r') as f:\n",
    "#     out_str = f.readlines()\n",
    "#     f.close()\n",
    "#   origin = []\n",
    "#   lemma = []\n",
    "#   pos = []\n",
    "#   idx = 0\n",
    "#   while idx < len(out_str):\n",
    "#     if out_str[idx].startswith(\"# text = \"):\n",
    "#       idx += 1\n",
    "#       lemma_tmp = []\n",
    "#       pos_tmp = []\n",
    "#       orig_tmp = []\n",
    "#       while out_str[idx] != '\\n':\n",
    "#         origi_i = out_str[idx].split('\\t')[1].lower()\n",
    "#         lemma_i = out_str[idx].split('\\t')[2]\n",
    "#         pos_tag = out_str[idx].split('\\t')[3]\n",
    "#         #print(lemma_i)\n",
    "#         orig_tmp.append(origi_i.replace('#', ''))\n",
    "#         lemma_tmp.append(lemma_i.replace('#', ''))\n",
    "#         pos_tmp.append(pos_tag)\n",
    "#         idx += 1\n",
    "#       origin.append(orig_tmp)\n",
    "#       lemma.append(lemma_tmp)\n",
    "#       pos.append(pos_tmp)\n",
    "#     else:\n",
    "#       idx += 1\n",
    "#   mask = []\n",
    "#   for s_idx in range(len(pos)):\n",
    "#     sent_pos = pos[s_idx]\n",
    "#     sent = lemma[s_idx]\n",
    "#     mask_tmp = []\n",
    "#     for tok_idx in range(len(sent)):\n",
    "#       if sent_pos[tok_idx] == 'NOUN':\n",
    "#         if sent[tok_idx] not in ['mm', 'cm', 'm', 'g', 'kg']:\n",
    "#           mask_tmp.append('T')\n",
    "#         else:\n",
    "#           mask_tmp.append('F')\n",
    "#       else:\n",
    "#         mask_tmp.append('F')\n",
    "#       # mask_tmp.append('T')\n",
    "#     mask.append(mask_tmp)\n",
    "    \n",
    "#   return origin, lemma, mask\n",
    "\n",
    "# def write_list(file_path, data_list):\n",
    "#   res_str = ''\n",
    "#   for item in data_list:\n",
    "#     res_str += ';'.join(item)\n",
    "#     res_str += '\\n'\n",
    "#   with open(file_path, 'w+') as f:\n",
    "#     f.write(res_str)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79756ad0-90e6-4370-90df-01ddd8f4b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        dicts = json.load(f).decode('utf8')\n",
    "        f.close()\n",
    "    return dicts\n",
    "\n",
    "def Mesh_dict(mesh_path):\n",
    "  mesh_dict = {}\n",
    "  with open(mesh_path, encoding='utf-8') as file1:\n",
    "    mem_string = \"\"\n",
    "    for line in file1:\n",
    "      if line == '\\n':\n",
    "        if mem_string.startswith('mesh:'):\n",
    "          med_list = mem_string.split(';')\n",
    "          try:\n",
    "            key = re.findall(r'mesh:[a-z]*[A-Z][0-9]+', med_list[0])[0]\n",
    "          except IndexError:\n",
    "            pass\n",
    "          mesh_dict[key] = {}\n",
    "          for i in med_list:\n",
    "            item = i.strip()\n",
    "            if item.startswith('skos:broader'):\n",
    "              mesh_dict[key]['broader'] = list()\n",
    "              mesh_dict[key]['broader'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "            if item.startswith('skos:prefLabel'):\n",
    "              # add other language\n",
    "              mesh_dict[key]['AllLabel'] = list()\n",
    "              if '\"@fi' in item:\n",
    "                mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@fi', item)[0][1:-4])\n",
    "              if '\"@en' in item:\n",
    "                mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@en', item)[0][1:-4])\n",
    "              if '\"@sv' in item:\n",
    "                mesh_dict[key]['AllLabel'].append(re.findall(r'\".*\"@sv', item)[0][1:-4])\n",
    "              try:\n",
    "                mesh_dict[key]['prefLabel'] = re.findall(r'\".*\"@fi', item)[0][1:-4]\n",
    "              except IndexError:\n",
    "                mesh_dict[key]['prefLabel'] = re.findall(r'\".*\"@en', item)[0][1:-4]\n",
    "            if item.startswith('skos:altLabel'):\n",
    "              mesh_dict[key]['altLabel'] = list()\n",
    "              alt_list = re.findall(r'\".*\"@fi', item)\n",
    "              for i in alt_list:\n",
    "                mesh_dict[key]['altLabel'].append(i[1:-4])\n",
    "            if item.startswith('skos:narrower'):\n",
    "              mesh_dict[key]['narrower'] = list()\n",
    "              mesh_dict[key]['narrower'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "            if item.startswith('skos:related'):\n",
    "              mesh_dict[key]['related'] = list()\n",
    "              mesh_dict[key]['related'].extend(re.findall(r'[a-z]*:[a-z]*[A-Z]*[0-9]+', item))\n",
    "        mem_string = \"\"\n",
    "        key = \"\"\n",
    "      else:\n",
    "        mem_string = mem_string + line\n",
    "        \n",
    "  return mesh_dict\n",
    "\n",
    "def Estab_rel(sum_dict):\n",
    "  rel_list = []\n",
    "  for k, v in sum_dict.items():\n",
    "    if 'broader' in list(v.keys()):\n",
    "      for b in v['broader']:\n",
    "        rel_list.append((b, k))\n",
    "    if 'narrower' in list(v.keys()):\n",
    "      for n in v['narrower']:\n",
    "        rel_list.append((k, n))\n",
    "          \n",
    "  return rel_list\n",
    "\n",
    "def traverse(hierarchy, graph, names):\n",
    "    for name in names:\n",
    "        hierarchy[name] = traverse({}, graph, graph[name])\n",
    "    return hierarchy\n",
    "\n",
    "def Build_graph(rel_list):\n",
    "  graph = {name: set() for tup in rel_list for name in tup}\n",
    "  has_parent = {name: False for tup in rel_list for name in tup}\n",
    "  for parent, child in rel_list:\n",
    "      graph[parent].add(child)\n",
    "      has_parent[child] = True\n",
    "\n",
    "  roots = [name for name, parents in has_parent.items() if not parents]\n",
    "  \n",
    "  res_dict = traverse({}, graph, roots)\n",
    "  \n",
    "  return res_dict, roots\n",
    "\n",
    "def flatten(d):    \n",
    "  res = []  # Result list\n",
    "  for key, val in d.items():\n",
    "    res.append(key)\n",
    "    res += flatten(val)\n",
    "    if d == {}:\n",
    "      res = d\n",
    "  return res\n",
    "\n",
    "def Flat_dict(res_dict):\n",
    "  flat_dict = {}\n",
    "  for k, v in res_dict.items():\n",
    "    flat_dict[k] = flatten(v)\n",
    "  return flat_dict\n",
    "\n",
    "def Retrieve_tree(label, f_dicts, m_dicts, r_nodes):\n",
    "  label4code = [i[0] for i in r_nodes if i[1] == label][0]\n",
    "  item_list = f_dicts[label4code]\n",
    "  label_list = [m_dicts[j]['prefLabel'] for j in item_list]\n",
    "  return label_list\n",
    "\n",
    "# compressed dictionary\n",
    "def create_new_dicts(flat_dict, mesh_dict):\n",
    "  label2code = {}\n",
    "  for k, v in mesh_dict.items():\n",
    "    if k != '':\n",
    "      label2code[v['prefLabel']] = k\n",
    "  # comp_dicts = {\n",
    "  #   'Anatomical-structure': ['tuki- ja liikuntaelimistö', 'kudokset', 'hermosto', 'purentaelimistö', 'veri- ja immuunijärjestelmät', 'sydän ja verenkiertoelimistö', 'ruuansulatuselimistö', 'hengityselimet', 'umpieritysjärjestelmä', 'aistinelimet', 'virtsa- ja sukupuolielimet', 'kehonalueet', 'iho ja ihon apuelimet'],\n",
    "  #   'Body-function-and-measurement': ['lisääntymis- ja virtsaelinten fysiologiset ilmiöt', 'muskuloskeletaaliset ja hermostolliset fysiologiset ilmiöt', 'ruuansulatuskanavan ja suun fysiologiset ilmiöt', 'ihovaipan fysiologiset ilmiöt', 'fysiologiset ilmiöt', 'verenkierto- ja hengitysfysiologia', 'aineenvaihdunta'],\n",
    "  #   'Medical-condition': ['purentaelimistön sairaudet', 'patologiset tilat, löydökset ja oireet', 'hermoston sairaudet', 'kasvaimet', 'ravitsemus- ja aineenvaihduntasairaudet', 'korva-, nenä- ja kurkkutaudit', 'miesten virtsa- ja sukupuolielinten sairaudet', 'iho- ja sidekudossairaudet', 'tuki- ja liikuntaelinten sairaudet', 'immuunijärjestelmän sairaudet', 'endokriiniset sairaudet', 'hengityselinten taudit', 'veri- ja lymfaattiset taudit', 'naisen virtsa- ja sukupuolielinten sairaudet ja raskauskomplikaatiot', 'synnynnäiset, perinnölliset ja vastasyntyneen taudit ja epämuodostumat', 'sydän- ja verisuonitaudit', 'infektiot', 'silmätaudit', 'haavat ja vammat'],\n",
    "  #   'Medical-device': ['tarvikkeet ja varusteet'],\n",
    "  #   'Medical-procedure': ['leikkaukset', 'diagnoosi', 'hoitomenetelmät', 'informaatiotiede', 'anestesia ja analgesia', 'tutkimusmenetelmät'],\n",
    "  #   'Medication': ['lääkevalmisteet', 'polysykliset yhdisteet', 'kemikaalien vaikutukset ja käyttötarkoitukset', 'heterosykliset yhdisteet', 'epäorgaaniset kemikaalit', 'orgaaniset kemikaalit']\n",
    "  # }\n",
    "    # 'Body-function-and-measurement': ['terveydenhuollon ammatit', 'ihmisten toiminnot', 'informaatiotiede', \n",
    "    # 'Medical-procedure': ['henkilöt',\n",
    "    comp_dicts = {\n",
    "    'Anatomical-structure': ['tuki- ja liikuntaelimistö', 'kudokset', 'hermosto', 'purentaelimistö', 'veri- ja immuunijärjestelmät', 'sydän ja verenkiertoelimistö', 'ruuansulatuselimistö', 'hengityselimet', 'umpieritysjärjestelmä', 'aistinelimet', 'virtsa- ja sukupuolielimet', 'kehonalueet', 'iho ja ihon apuelimet'],\n",
    "    'Body-function-and-measurement': ['lisääntymis- ja virtsaelinten fysiologiset ilmiöt', 'muskuloskeletaaliset ja hermostolliset fysiologiset ilmiöt', 'ruuansulatuskanavan ja suun fysiologiset ilmiöt', 'ihovaipan fysiologiset ilmiöt', 'fysiologiset ilmiöt', 'verenkierto- ja hengitysfysiologia', 'aineenvaihdunta'],\n",
    "    'Medical-condition': ['purentaelimistön sairaudet', 'patologiset tilat, löydökset ja oireet', 'hermoston sairaudet', 'kasvaimet', 'ravitsemus- ja aineenvaihduntasairaudet', 'korva-, nenä- ja kurkkutaudit', 'virtsa- ja sukupuolielinten sairaudet', 'iho- ja sidekudossairaudet', 'tuki- ja liikuntaelinten sairaudet', 'immuunijärjestelmän sairaudet', 'endokriiniset sairaudet', 'hengityselinten taudit', 'veri- ja lymfaattiset taudit', 'synnynnäiset, perinnölliset ja vastasyntyneen taudit ja epämuodostumat', 'sydän- ja verisuonitaudit', 'infektiot', 'silmätaudit', 'haavat ja vammat'],\n",
    "    'Medical-device': ['tarvikkeet ja varusteet'],\n",
    "    'Medical-procedure': ['leikkaukset', 'diagnoosi', 'hoitomenetelmät', 'anestesia ja analgesia', 'tutkimusmenetelmät'],\n",
    "    'Medication': ['henkilöt','lääkevalmisteet', 'polysykliset yhdisteet', 'kemikaalien vaikutukset ja käyttötarkoitukset', 'heterosykliset yhdisteet', 'epäorgaaniset kemikaalit', 'orgaaniset kemikaalit']\n",
    "  }\n",
    "  # then check each element in the medical dictionary\n",
    "  sub_item_collect = []\n",
    "  code_dicts = {}\n",
    "  for k, v in comp_dicts.items():\n",
    "    # print(v)\n",
    "    code_dicts[k] = []\n",
    "    for item in v:\n",
    "      code_dicts[k].append(label2code[item])\n",
    "    # append separate value list\n",
    "    sub_item_collect += v\n",
    "  # size of updated medical dictionary\n",
    "  size_dicts = len(sub_item_collect)\n",
    "  # use flatten dictionary to construct new dictionary\n",
    "  new_dicts = {}\n",
    "  for k, v in code_dicts.items():\n",
    "    new_dicts[k] = []\n",
    "    for item in v:\n",
    "      elements = flat_dict[item]\n",
    "      new_dicts[k] += elements\n",
    "      new_dicts[k] += [item]\n",
    "\n",
    "  return new_dicts\n",
    "\n",
    "def label_pooling(string):\n",
    "  str_set = []\n",
    "  str_element = string.split(';')\n",
    "  for i in str_element:\n",
    "    str_set.append(i.lower())\n",
    "  \n",
    "  return str_set\n",
    "\n",
    "def token_mapping(source_tok, sub_dict):\n",
    "  matched_num = 0\n",
    "  matched_tok = []\n",
    "\n",
    "  if ' ' in source_tok:\n",
    "    source = source_tok.split(' ')\n",
    "  else:\n",
    "    source = [source_tok]\n",
    "  source_len = np.sum(np.array([len(i) for i in source]))\n",
    "  for item in sub_dict:\n",
    "    if ' ' in item:\n",
    "      piece = item.split(' ')\n",
    "    else:\n",
    "      piece = [item]\n",
    "    # num of terms is equal\n",
    "    if len(piece) == len(source):\n",
    "      matched_count = 0\n",
    "      for p in range(len(piece)):\n",
    "        # len of each piece >= len of source piece\n",
    "        if len(piece[p]) >= len(source[p]):\n",
    "          for t in range(len(source[p])):\n",
    "            if piece[p][t] == source[p][t]:\n",
    "              matched_count += 1\n",
    "            else:\n",
    "              break\n",
    "          if matched_count == source_len:\n",
    "            matched_num += 1\n",
    "            matched_tok.append(item)\n",
    "  return matched_num, matched_tok\n",
    "\n",
    "def candidates(source, dicts):\n",
    "  can_len = []\n",
    "  can = []\n",
    "  for k, v in dicts.items():\n",
    "    num, tok = token_mapping(source, v)\n",
    "    can_len.append(num)\n",
    "    can.append(tok)\n",
    "  return can_len, can\n",
    "\n",
    "def cal_score(source, cans):\n",
    "  scores = []\n",
    "  for cat in cans:\n",
    "    score = []\n",
    "    for item in cat:\n",
    "      score.append(fuzz.ratio(source, item))\n",
    "    if len(score) == 0:\n",
    "      scores.append(0)\n",
    "    else:\n",
    "      scores.append(np.max(np.array(score)))\n",
    "  return scores\n",
    "\n",
    "def label_assign(s1, s2):\n",
    "  s = []\n",
    "  for i in range(len(s1)):\n",
    "    if s1[i] > s2[i]:\n",
    "      s.append(s1[i])  \n",
    "    else:\n",
    "      s.append(s2[i])  \n",
    "  s = np.array(s)\n",
    "  if np.max(s) == 0:\n",
    "    max_idx = -1\n",
    "    max_value = 0\n",
    "  else:\n",
    "    max_value = 0\n",
    "    max_idx = 0\n",
    "    for z in range(len(s)):\n",
    "      if s[z] > max_value:\n",
    "        max_idx = z\n",
    "        max_value = s[z]\n",
    "      # if the values are equal, then comparison on two list\n",
    "      elif s[z] == max_value:\n",
    "        s1_z = s1[z]\n",
    "        s1_m = s1[max_idx]\n",
    "        s2_z = s2[z]\n",
    "        s2_m = s2[max_idx]\n",
    "        # one of list, the z win\n",
    "        if s1_z > s1_m and s2_z == s2_m:\n",
    "          max_idx = z\n",
    "        elif s1_z == s1_m and s2_z > s2_m:\n",
    "          max_idx = z\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "  return max_idx, max_value\n",
    "\n",
    "def create_res_dicts(a_dict, m_dict):\n",
    "  res_dicts = {}\n",
    "  res_dicts_alt = {}\n",
    "  for k, v in a_dict.items():\n",
    "    md_items = v\n",
    "    label_lake = []\n",
    "    label_lake_alt = []\n",
    "    for i in md_items:\n",
    "        cur_item = m_dict[i]\n",
    "        if 'altLabel' in cur_item.keys():\n",
    "            cur_alt = cur_item['altLabel']\n",
    "        else:\n",
    "            cur_alt = []\n",
    "        cur_pref = cur_item['prefLabel']\n",
    "        cur_all = cur_item['AllLabel']\n",
    "        if len(cur_alt) > 0:\n",
    "            label_lake_alt += cur_alt\n",
    "        if cur_pref != '':\n",
    "          label_lake.append(cur_pref)\n",
    "        if len(cur_all) > 0:\n",
    "          label_lake += cur_all\n",
    "    label_lake = list(set(label_lake))\n",
    "    main_match_string = ';'.join(label_lake)\n",
    "    res_dicts[k] = label_pooling(main_match_string)\n",
    "\n",
    "    label_lake_alt = list(set(label_lake_alt))\n",
    "    main_match_string_alt = ';'.join(label_lake_alt)\n",
    "    res_dicts_alt[k] = label_pooling(main_match_string_alt)\n",
    "\n",
    "  return res_dicts, res_dicts_alt\n",
    "\n",
    "\n",
    "\n",
    "def lemma_tokens(input_path):\n",
    "  with open(input_path, 'r') as f:\n",
    "    out_str = f.readlines()\n",
    "    f.close()\n",
    "  origin = []\n",
    "  lemma = []\n",
    "  pos = []\n",
    "  case = []\n",
    "  idx = 0\n",
    "  while idx < len(out_str):\n",
    "    if out_str[idx].startswith(\"# text = \"):\n",
    "      idx += 1\n",
    "      lemma_tmp = []\n",
    "      pos_tmp = []\n",
    "      orig_tmp = []\n",
    "      case_tmp = []\n",
    "      while out_str[idx] != '\\n':\n",
    "        trg_str = out_str[idx].split('\\t')\n",
    "        # raise ValueError('1')\n",
    "        origi_i = trg_str[1].lower()\n",
    "        lemma_i = trg_str[2]\n",
    "        pos_tag = trg_str[3]\n",
    "        \n",
    "        #print(lemma_i)\n",
    "        orig_tmp.append(origi_i.replace('#', ''))\n",
    "        lemma_tmp.append(lemma_i)\n",
    "        pos_tmp.append(pos_tag)\n",
    "        case_tmp.append(trg_str[5])\n",
    "        idx += 1\n",
    "      origin.append(orig_tmp)\n",
    "      lemma.append(lemma_tmp)\n",
    "      pos.append(pos_tmp)\n",
    "      case.append(case_tmp)\n",
    "    else:\n",
    "      idx += 1\n",
    "  mask = []\n",
    "  for s_idx in range(len(pos)):\n",
    "    sent_pos = pos[s_idx]\n",
    "    sent = lemma[s_idx]\n",
    "    sent_case = case[s_idx]\n",
    "    mask_tmp = []\n",
    "    for tok_idx in range(len(sent)):\n",
    "      if sent_pos[tok_idx] == 'NOUN':\n",
    "        # print(sent[tok_idx])\n",
    "        # print(len(sent[tok_idx]))\n",
    "        # print(sent[tok_idx].isalpha())\n",
    "        # print(sent[tok_idx] not in stop_list)\n",
    "        # raise ValueError('1')\n",
    "        # print(sent[tok_idx])\n",
    "        if sent[tok_idx] not in stop_list and sent[tok_idx].replace('#', '').isalpha() and len(sent[tok_idx]) > 2:\n",
    "          # print(sent[tok_idx])\n",
    "          # print(sent[tok_idx].isalpha())\n",
    "          # raise ValueError('1')\n",
    "          mask_tmp.append('T')\n",
    "        # elif '_' in sent_case[tok_idx]:# or 'Abbr=Yes' in sent_case[tok_idx]:\n",
    "        #   mask_tmp.append('F')\n",
    "        else:\n",
    "          mask_tmp.append('F')\n",
    "      else:\n",
    "        mask_tmp.append('F')\n",
    "      # mask_tmp.append('T')\n",
    "    mask.append(mask_tmp)\n",
    "    \n",
    "  return origin, lemma, mask, case, pos\n",
    "\n",
    "def write_list(file_path, data_list):\n",
    "  res_str = ''\n",
    "  for item in data_list:\n",
    "    res_str += ';'.join(item)\n",
    "    res_str += '\\n'\n",
    "  with open(file_path, 'w+') as f:\n",
    "    f.write(res_str)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "# def expand_item(dict_1, dict_2):\n",
    "#     new_res_list = []\n",
    "#     for m in dict_1:\n",
    "#         #non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "#         non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()) if item != '']\n",
    "#         new_res_list.append(' '.join(non_alpha_list))\n",
    "#         new_res_list += non_alpha_list\n",
    "    \n",
    "#     for m in dict_2:\n",
    "#         #non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "#         non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()) if item != '']\n",
    "#         new_res_list.append(' '.join(non_alpha_list))\n",
    "#         new_res_list += non_alpha_list\n",
    "        \n",
    "#     return list(set(new_res_list))\n",
    "\n",
    "def expand_item(dict_1, dict_2):\n",
    "    alpha_dict_1 = {}\n",
    "    alpha_dict_2 = {}\n",
    "    alpha_dict_3 = {}\n",
    "    # chara_ = []\n",
    "    for m in dict_1:\n",
    "        #print(m)\n",
    "        #print(re.sub(r'[^a-z]', ' ', m.lower()))\n",
    "        non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "        # alpha_dict_1 = {}\n",
    "        # alpha_dict_2 = {}\n",
    "        # alpha_dict_3 = {}\n",
    "        # non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()) if item != '']\n",
    "        # print(non_alpha_list)\n",
    "        # raise ValueError('1')\n",
    "        if len(non_alpha_list) == 1:\n",
    "            #print(non_alpha_list)\n",
    "            #raise ValueError('1')\n",
    "            # chara_.append(non_alpha_list[0][0])\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_1.keys()):\n",
    "                alpha_dict_1[tok_[0]] = set()\n",
    "            alpha_dict_1[tok_[0]].add(tok_)\n",
    "        elif len(non_alpha_list) == 2:\n",
    "            # new_res_list_2.append(' '.join(non_alpha_list))\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_2.keys()):\n",
    "                alpha_dict_2[tok_[0]] = set()\n",
    "            alpha_dict_2[tok_[0]].add(tok_)\n",
    "        else:\n",
    "            # new_res_list_3.append(' '.join(non_alpha_list))\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_3.keys()):\n",
    "                alpha_dict_3[tok_[0]] = set()\n",
    "            alpha_dict_3[tok_[0]].add(tok_)\n",
    "        # new_res_list += non_alpha_list\n",
    "    \n",
    "    for m in dict_2:\n",
    "        #print(m)\n",
    "        #print(re.sub(r'[^a-z]', ' ', m.lower()))\n",
    "        non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "        # alpha_dict_1 = {}\n",
    "        # alpha_dict_2 = {}\n",
    "        # alpha_dict_3 = {}\n",
    "        #non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()) if item != '']\n",
    "        # print(non_alpha_list)\n",
    "        # raise ValueError('1')\n",
    "        if len(non_alpha_list) == 1:\n",
    "            #print(non_alpha_list)\n",
    "            #raise ValueError('1')\n",
    "            # chara_.append(non_alpha_list[0][0])\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_1.keys()):\n",
    "                alpha_dict_1[tok_[0]] = set()\n",
    "            alpha_dict_1[tok_[0]].add(tok_)\n",
    "        elif len(non_alpha_list) == 2:\n",
    "            # new_res_list_2.append(' '.join(non_alpha_list))\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_2.keys()):\n",
    "                alpha_dict_2[tok_[0]] = set()\n",
    "            alpha_dict_2[tok_[0]].add(tok_)\n",
    "        else:\n",
    "            # print(non_alpha_list)\n",
    "            # raise ValueError('1')\n",
    "            # new_res_list_3.append(' '.join(non_alpha_list))\n",
    "            tok_ = ' '.join(non_alpha_list)\n",
    "            if tok_[0] not in list(alpha_dict_3.keys()):\n",
    "                alpha_dict_3[tok_[0]] = set()\n",
    "            alpha_dict_3[tok_[0]].add(tok_)\n",
    "        # new_res_list += non_alpha_list\n",
    "    # print(set(chara_))\n",
    "    #raise ValueError('1')\n",
    "    return [alpha_dict_1, alpha_dict_2, alpha_dict_3]\n",
    "\n",
    "def expand_item_pool(dict_1, dict_2):\n",
    "    alpha_dict_1 = {}\n",
    "    # alpha_dict_2 = {}\n",
    "    # alpha_dict_3 = {}\n",
    "    # chara_ = []\n",
    "    for m in dict_1:\n",
    "        non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "        # tok_ = ' '.join(non_alpha_list)\n",
    "        for tok_ in non_alpha_list:\n",
    "            if tok_[0] not in list(alpha_dict_1.keys()):\n",
    "                alpha_dict_1[tok_[0]] = set()\n",
    "            if len(tok_) >= 2:\n",
    "                alpha_dict_1[tok_[0]].add(tok_)\n",
    "    \n",
    "    for m in dict_2:\n",
    "        #print(m)\n",
    "        #print(re.sub(r'[^a-z]', ' ', m.lower()))\n",
    "        non_alpha_list = [item for item in re.sub(r'[^a-z]', ' ', m.lower()).split() if item != '']\n",
    "        # tok_ = ' '.join(non_alpha_list)\n",
    "        for tok_ in non_alpha_list:\n",
    "            if tok_[0] not in list(alpha_dict_1.keys()):\n",
    "                alpha_dict_1[tok_[0]] = set()\n",
    "            if len(tok_) >= 2:\n",
    "                alpha_dict_1[tok_[0]].add(tok_)\n",
    "    # print(set(chara_))\n",
    "    #raise ValueError('1')\n",
    "    return alpha_dict_1\n",
    "\n",
    "def reverse_dict():\n",
    "    # reverse dict\n",
    "\n",
    "    name_list = ['Anatomical-structure', 'Body-function-and-measurement', 'Medical-condition', 'Medical-device', 'Medical-procedure', 'Medication']\n",
    "    # sets = 'dict_alt_'\n",
    "    sets = 'dict_'\n",
    "\n",
    "    dict_ = {}\n",
    "    for name in name_list:\n",
    "        file_name_1 = sets + name + '.txt'\n",
    "        read_path = '../data/dict/post/processed/{}'.format(file_name_1)\n",
    "        res_list = []\n",
    "        with open(read_path, 'r') as f:\n",
    "            for i in f.readlines():\n",
    "                res_list.append(i.strip())\n",
    "            f.close()\n",
    "        dict_[name] = res_list\n",
    "\n",
    "    # sets = 'dict_alt_'\n",
    "\n",
    "    dict_alt_ = {}\n",
    "    sets = 'dict_alt_'\n",
    "\n",
    "    for name in name_list:\n",
    "        file_name_1 = sets + name + '.txt'\n",
    "        read_path = '../data/dict/post/processed/{}'.format(file_name_1)\n",
    "        res_list = []\n",
    "        with open(read_path, 'r') as f:\n",
    "            for i in f.readlines():\n",
    "                res_list.append(i.strip())\n",
    "            f.close()\n",
    "        dict_alt_[name] = res_list\n",
    "    \n",
    "    return dict_, dict_alt_\n",
    "\n",
    "def beam_mapping(source_ele, r_dicts, r_dicts_alt, window, mask_list, check_thres):\n",
    "  \n",
    "  # print(source)\n",
    "  labels = []\n",
    "  # label_space =  list(r_dicts.keys())\n",
    "  ratio_thres = 0.5\n",
    "  # if ratio_score is too low --> subwording --> lose too much information\n",
    "  \n",
    "  # source_ele = source#.split(' ')\n",
    "  source_len = len(source_ele)\n",
    "  idx = 0\n",
    "    \n",
    "  new_source_ele = []\n",
    "  for source in source_ele:\n",
    "        # print(source)\n",
    "        if '#' in source:\n",
    "            source_list = source.split('#')\n",
    "            # print(source_list)\n",
    "            ratio_score = len(source_list[0])/len(''.join(source_list))\n",
    "            # print(ratio_score)\n",
    "            if ratio_score < ratio_thres:\n",
    "                source = ''.join(source_list)\n",
    "            else:\n",
    "                source = source_list[0]\n",
    "            # print(ratio_score)\n",
    "            # print(source)\n",
    "        new_source_ele.append(source)\n",
    "  source_ele = new_source_ele\n",
    "    # raise ValueError(1)\n",
    "    \n",
    "  name_list = ['Anatomical-structure', 'Body-function-and-measurement', 'Medical-condition', 'Medical-device', 'Medical-procedure', 'Medication']\n",
    "        \n",
    "  res_dict = {}\n",
    "\n",
    "  for i in name_list:\n",
    "    res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "    # res = expand_item_pool(r_dicts[i], r_dicts_alt[i])\n",
    "    # print(res[0])\n",
    "    # raise ValueError('1')\n",
    "    # print(len(res[1]))\n",
    "    # print(len(res[2]))\n",
    "    # print('-'*10)\n",
    "    res_dict[i] = res\n",
    "\n",
    "  # raise ValueError('1')\n",
    "  mask_list[0]\n",
    "  while idx < source_len:\n",
    "    if mask_list[idx] == 'T':\n",
    "      if (source_len - idx) >= window:\n",
    "        step = window\n",
    "      else:\n",
    "        step = source_len - idx\n",
    "      candidate_idx = []\n",
    "      candidate_value = []\n",
    "      for shift in range(1, step+1):\n",
    "        term = source_ele[idx:idx + shift]\n",
    "        term_str = ' '.join(term)\n",
    "        # !!! original !!!\n",
    "        # _, candidate = candidates(term_str, r_dicts)\n",
    "        # _, candidate_alt = candidates(term_str, r_dicts_alt)\n",
    "        # s1 = cal_score(term_str, candidate)\n",
    "        # s2 = cal_score(term_str, candidate_alt)\n",
    "        # max_idx, max_value = label_assign(s1, s2)\n",
    "        # !!! no accelerating !!!\n",
    "        # source = term_str\n",
    "        # score_list = []\n",
    "        # for i in name_list:\n",
    "        #     res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "        #     max_val = 0\n",
    "        #     # max_idx = 0\n",
    "        #     for j in range(len(res)):\n",
    "        #         item = res[j]\n",
    "        #         if fuzz.ratio(source, item) > max_val:\n",
    "        #             max_val = fuzz.ratio(source, item)\n",
    "        #             # max_idx = j\n",
    "        #     score_list.append(max_val)\n",
    "        # !!! accelerating\n",
    "        source = term_str\n",
    "        score_list = []\n",
    "\n",
    "        for k, v in res_dict.items():\n",
    "            max_val = 0\n",
    "            # max_idx = 0\n",
    "            # print(v)\n",
    "            # raise ValueError(1)\n",
    "            # fir_tok = source[0]\n",
    "            # print(v[0])\n",
    "            \n",
    "            # if fir_tok in v[0]:\n",
    "            # -----------------------\n",
    "            # code below --> map tokens in phrase\n",
    "            if len(source.split()) == 1:\n",
    "                if source[0] in list(v[0].keys()):\n",
    "                    v_list = v[0][source[0]]\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            elif len(source.split()) == 2:\n",
    "                if source[0] in list(v[1].keys()):\n",
    "                    v_list = v[1][source[0]]\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            elif len(source.split()) == 3:\n",
    "                if source[0] in list(v[2].keys()):\n",
    "                    v_list = v[2][source[0]]\n",
    "                    # print(v_list)\n",
    "                    # raise ValueError(1)\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            v_list_ = list(v_list)\n",
    "            # -----------------------\n",
    "            # code below --> pooling all phrases\n",
    "            # v_list_ = \n",
    "            # for j in range(len(v_list_)):\n",
    "            #     item = v_list_[j]\n",
    "            #     item_list = item.split()\n",
    "#             source_list = source.split()\n",
    "#             # print(source)\n",
    "#             # map 1 token; 2 token; 3 token\n",
    "#             sum_max_val = 0\n",
    "#             for z in range(len(source_list)):\n",
    "#                 source_tok = source_list[z]\n",
    "#                 if source_tok[0] in list(v.keys()):\n",
    "#                     item_list = v[source_tok[0]]\n",
    "#                 else:\n",
    "#                     item_list = []\n",
    "#                 # print(item_list)\n",
    "#                 # raise ValueError(1)\n",
    "#                 tmp_max_val = 0\n",
    "#                 for item_tok in item_list:\n",
    "#                     # print(source_list)\n",
    "#                     # print(source_tok)\n",
    "#                     # print(item_list)\n",
    "#                     # raise ValueError(1)\n",
    "#                     # for zz in \n",
    "#                     map_len = int(len(source_tok)*check_thres)\n",
    "#                     # print(item)\n",
    "#                     if len(item_tok) < map_len:\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         # if fuzz.ratio(source, item) > max_val:\n",
    "#                         #     max_val = fuzz.ratio(source, item)\n",
    "#                         if source_tok[:map_len] == item_tok[:map_len]:\n",
    "#                             # print(source[:map_len])\n",
    "#                             # print(item)\n",
    "#                             # print(source_tok)\n",
    "#                             # print(item_tok)\n",
    "#                             # raise ValueError(1)\n",
    "#                             IoU_score = map_len/len(item_tok)\n",
    "#                             # print(IoU_score)\n",
    "#                             # raise ValueError(1)\n",
    "#                             if IoU_score > sum_max_val:\n",
    "#                                 tmp_max_val = IoU_score\n",
    "#                 sum_max_val += tmp_max_val\n",
    "#             sum_max_val = sum_max_val#/len(source_list)\n",
    "#             if sum_max_val > max_val:\n",
    "#                 max_val = sum_max_val\n",
    "                \n",
    "#                 # fuzz_score = fuzz.ratio(source, item)\n",
    "#                 # # print(source)\n",
    "#                 # # print(item)\n",
    "#                 # # raise ValueError('1')\n",
    "#                 # if fuzz_score > max_val:\n",
    "#                 #     max_val = fuzz_score\n",
    "#                 #     # max_idx = j\n",
    "#             # score_list.append(max_val/len(source_list))\n",
    "#             score_list.append(max_val)\n",
    "            # -------------------------\n",
    "        # print(source)\n",
    "        # print('_'*10)\n",
    "        # print(score_list)\n",
    "        # raise ValueError(1)\n",
    "            # map_thres = 0.9\n",
    "            # print(source)\n",
    "            # map_len = int(len(source)*check_thres)\n",
    "            # print(map_len)\n",
    "            # print(source[:map_len])\n",
    "            # print(len(source.split()))\n",
    "            # raise ValueError(1)\n",
    "            for j in range(len(v_list_)):\n",
    "                item = v_list_[j]\n",
    "                item_list = item.split()\n",
    "                source_list = source.split()\n",
    "                # map 1 token; 2 token; 3 token\n",
    "                sum_max_val = 0\n",
    "                # print(source_list)\n",
    "                # print(item_list)\n",
    "                # raise ValueError(1)\n",
    "                for z in range(len(source_list)):\n",
    "                    source_tok = source_list[z]\n",
    "                    item_tok = item_list[z]\n",
    "                    map_len = int(len(source_tok)*check_thres)\n",
    "                    # print(item)\n",
    "                    if len(item_tok) < map_len:\n",
    "                        break\n",
    "                    else:\n",
    "                        # if fuzz.ratio(source, item) > max_val:\n",
    "                        #     max_val = fuzz.ratio(source, item)\n",
    "                        # raise ValueError(1)\n",
    "                        if source_tok[:map_len] == item_tok[:map_len]:\n",
    "                            # print(source[:map_len])\n",
    "                            # print(item)\n",
    "                            IoU_score = map_len/len(item)\n",
    "                            if IoU_score > sum_max_val:\n",
    "                                sum_max_val = IoU_score\n",
    "                                \n",
    "                if sum_max_val > max_val:\n",
    "                    max_val = sum_max_val\n",
    "                \n",
    "                # fuzz_score = fuzz.ratio(source, item)\n",
    "                # # print(source)\n",
    "                # # print(item)\n",
    "                # # raise ValueError('1')\n",
    "                # if fuzz_score > max_val:\n",
    "                #     max_val = fuzz_score\n",
    "                #     # max_idx = j\n",
    "            score_list.append(max_val/len(source_list))\n",
    "        # raise ValueError(1)\n",
    "        # print(source)\n",
    "        # print(source)\n",
    "        # print(score_list)\n",
    "        # print(term_str)\n",
    "        # score_list = []\n",
    "        # for i in label_space:\n",
    "        #     res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "        #     max_val = 0\n",
    "        #     # max_idx = 0\n",
    "        #     for j in range(len(res)):\n",
    "        #         item = res[j]\n",
    "        #         if fuzz.ratio(term_str, item) > max_val:\n",
    "        #             max_val = fuzz.ratio(source, item)\n",
    "        #             # max_idx = j\n",
    "        #     score_list.append(max_val)\n",
    "        # print(score_list)\n",
    "        # max_score = np.max(score_list)\n",
    "        candidate_idx.append(np.argmax(score_list))\n",
    "        candidate_value.append(np.max(score_list))\n",
    "        \n",
    "      # raise ValueError(1)\n",
    "      if np.max(np.array(candidate_value)) > 0.5:\n",
    "        # the position of best matching\n",
    "        max_pos = np.argmax(np.array(candidate_value))\n",
    "        # the label index of best matching\n",
    "        label_idx = candidate_idx[max_pos]\n",
    "        win_shift = max_pos + 1\n",
    "        idx += win_shift\n",
    "        label = name_list[label_idx]\n",
    "        if win_shift == 1:\n",
    "          labels.append('B-' + label)\n",
    "        else:\n",
    "          labels.append('B-' + label)\n",
    "          for i in range(win_shift - 1):\n",
    "            labels.append('I-' + label)\n",
    "      else:\n",
    "        labels.append('O')\n",
    "        idx += 1\n",
    "    else:\n",
    "      labels.append('O')\n",
    "      idx += 1\n",
    "\n",
    "  return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0f93f-a4b9-4984-a356-3e07c9494ba9",
   "metadata": {
    "gather": {
     "logged": 1668764495694
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    sets = 'test'\n",
    "    input_path = '../data/lemma_data/{}/1.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/1.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    # raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        # s_idx = 1\n",
    "        # labels_tmp_list = []\n",
    "        # for ix in range(50):\n",
    "        #     lemma[s_idx] = [item[ix] for item in res_list] + ['.']\n",
    "        #     labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        #     # print(labels_tmp)\n",
    "        #     labels_tmp_list.append(labels_tmp)\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        print(labels_tmp)\n",
    "        raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42571c77-7985-4f6b-b4d7-401f7c205a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Anatomical-structure', 'Body-function-and-measurement', 'Medical-condition', 'Medical-device', 'Medical-procedure', 'Medication']\n",
    "bio_label = []\n",
    "for i in label:\n",
    "    bio_label.append('B-' + i)\n",
    "    bio_label.append('I-' + i)\n",
    "bio_label.append('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408b397-0021-47af-b4e8-cd1ca6aad3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_label = {}\n",
    "for idx in range(len(bio_label)):\n",
    "    idx_2_label[idx] = bio_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d832fd-69d1-4b9e-8ce3-b0e4a69149b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_idx = {v:k for k,v in idx_2_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26dfbd-3a84-438b-a815-ae8a5ff13b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = np.zeros([len(labels_tmp_list[0]), len(idx_2_label)])\n",
    "for i in range(len(labels_tmp_list)):\n",
    "    item = labels_tmp_list[i]\n",
    "    for j in range(len(item)):\n",
    "        if item[j] != 'O':\n",
    "            mx[j][label_2_idx[item[j]]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0f93e-5818-4f8b-a0f8-8a10e7566121",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf68f19-2acf-4d1e-a5f7-db18367ad51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ed078-7370-46ed-94b8-41ed84ab61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9549f08-a465-42e9-a410-4883e8695a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_tmp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6cfcef-7a25-4b82-922f-46ec5579d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6bc3c-2040-421e-86ce-59e91625bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979966d0-5c59-453a-856f-31535d39ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ce5c9-e405-44ef-97f2-243b5b2512ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d2377-fc04-415c-ae8d-6179d892b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = '../tools/bert-base-finnish-uncased-v1'\n",
    "mlm_model_path = '../save_models/pretrain_weight/checkpoint-100000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504138f2-fce1-405a-93d4-b8c489079c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    res_topk = []\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        # print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "        # print(predicted_token)\n",
    "        # choose the best token\n",
    "        res_topk.append(predicted_token)\n",
    "    return res_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc16ce-22d6-476f-84de-f20b17084b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "model = BertForMaskedLM.from_pretrained(mlm_model_path)\n",
    "model.eval()\n",
    "# predict_masked_sent(\"My [MASK] is so cute.\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef4552-aaa9-40d3-acb6-da0918b6e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"\"\n",
    "input_list = input_str.split()[:-1]\n",
    "res_list = []\n",
    "for i in range(len(input_list)):\n",
    "    mask_tok = input_list[i]\n",
    "    tmp_str = ' '.join(input_list) + '.'\n",
    "    res_str = tmp_str.replace(mask_tok, '[MASK]')\n",
    "    res_list.append(predict_masked_sent(res_str, top_k=20))\n",
    "res_list\n",
    "    # print('-'*10)\n",
    "# predict_masked_sent(, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff206-0323-4bbb-a344-d188e4350b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3263dda-d8e3-4b14-90bb-e76b788d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mask[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2647f8-a72e-49e2-a4a2-6f8bc3547ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafe729-b051-421b-8cb8-be6e02f4e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9e423-399e-4dbc-be13-b45bf654e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[0] for item in res_list] + ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f490c13-8a83-411a-8ffe-e379b6f2c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "\n",
    "# # trg_dict = res_dicts_alt\n",
    "# # sets = 'dict_alt_'\n",
    "# # trg_dict = res_dicts\n",
    "# # sets = 'dict_'\n",
    "\n",
    "# for sets in ['dict_alt_', 'dict_']:\n",
    "#     if sets == 'dict_alt_':\n",
    "#         trg_dict = res_dicts_alt\n",
    "#     else:\n",
    "#         trg_dict = res_dicts\n",
    "#     name_list = list(trg_dict.keys())\n",
    "#     for name in name_list:\n",
    "#         file_tail = sets + name + '.txt'\n",
    "#         write_path = '../data/dict/pre/{}'.format(file_tail)\n",
    "#         with open(write_path, 'w+') as f:\n",
    "#             f.write('\\n'.join(trg_dict[name])) \n",
    "#             f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ecebf-c39e-4cc2-9408-947e6c579d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "human_test_path = '../data/weak_data/KIR/test/tag1.txt'\n",
    "\n",
    "human_list = []\n",
    "with open(human_test_path, 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        human_list.append(i.strip().split())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb92ee-6dab-46e7-8d30-57f2821361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = ['O', 'O', 'O', 'O', 'O', 'B-Body-function-and-measurement', 'B-Medical-condition', 'O', 'B-Body-function-and-measurement', 'O', 'O', 'O']\n",
    "idx = 0\n",
    "d = {'origin':origin[idx], 'lemma':lemma[idx], 'human':human_list[idx], 'ham':ham}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1557b-6211-4efa-a8e4-c58520c80ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "case[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631d3a7-6ee5-468e-b9a1-47333601d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _\n",
    "# Person\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a6ee4-c7ac-485b-8525-e0e1731036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tense=Pres\n",
    "# Person=3\n",
    "# Mood=Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf236f-a558-4b4f-bf2c-681aee0de993",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(len(pos[:thre])) if pos[:thre][i] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52b6b9-1cd1-4db2-aada-74d3fd2988c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[:thre][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc75a5a-f627-4f16-bdf6-4b956796f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ = [j for i in origin[:thre] for j in i]\n",
    "pos_ = [j for i in pos[:thre] for j in i]\n",
    "human_ = [j for i in human_list[:thre] for j in i]\n",
    "case_ = [j for i in case[:thre] for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbd310-d3df-4840-918b-036e54df096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_idx = [i for i in range(len(pos_)) if pos_[i] == 'NOUN']\n",
    "filter_idx_1 = [i for i in range(len(token_)) if pos_[i] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ea9db-6ca9-4ae2-a181-2b8741c1c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "thre = 100\n",
    "d = {'token': [token_[j] for j in filter_idx], 'pos': [pos_[j] for j in filter_idx], 'human': [human_[j] for j in filter_idx], 'case':[case_[j] for j in filter_idx]}\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "o_case_list = []\n",
    "noo_case_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if row['human'] != 'O':\n",
    "        noo_case_list.append(row['case'])\n",
    "    else:\n",
    "        o_case_list.append(row['case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bb6ef-a8bf-4698-a54e-dc9e22dc2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(o_case_list) - set(noo_case_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22953133-277c-4eee-8b87-b1f54661bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ebc8f-4638-406f-ae11-e63498c55348",
   "metadata": {
    "gather": {
     "logged": 1668764517287
    }
   },
   "outputs": [],
   "source": [
    "row['case']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399010fc-1fbc-4161-87c1-994b821e5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2code = {}\n",
    "code2label = {}\n",
    "for k, v in mesh_dict.items():\n",
    "    if k != '':\n",
    "        label2code[v['prefLabel']] = k\n",
    "        code2label[k] = v['prefLabel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec6304-005d-4edc-940b-11bc57f47a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "for k, v in flat_dict.items():\n",
    "    name_dict[code2label[k]] = [code2label[i] for i in list(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60a6ac-3dfc-4397-bb81-66125cb0e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in name_dict.items():\n",
    "    if 'vaikeutunut' in v:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61edcc-8931-4503-8e6d-7906e17fcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360e76c-1324-41a4-91b3-5291797b1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['Anatomical-structure', 'Body-function-and-measurement', 'Medical-condition', 'Medical-device', 'Medical-procedure', 'Medication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2410df-0638-4fea-834b-6cb49ae1bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in name_list:\n",
    "    res = expand_item(res_dicts[i], res_dicts_alt[i])\n",
    "    res_dict[i] = res\n",
    "    \n",
    "source = 'haavakontrolliin'\n",
    "score_list = []\n",
    "\n",
    "for k, v in res_dict.items():\n",
    "    max_val = 0\n",
    "    # max_idx = 0\n",
    "    if len(source.split()) == 1:\n",
    "        v_list = v[0]\n",
    "    elif len(source.split()) == 2:\n",
    "        v_list = v[1]\n",
    "    else:\n",
    "        v_list = v[2]\n",
    "    for j in range(len(v_list)):\n",
    "        item = v_list[j]\n",
    "        fuzz_score = fuzz.ratio(source, item)\n",
    "        if fuzz_score > max_val:\n",
    "            max_val = fuzz_score\n",
    "            # max_idx = j\n",
    "    score_list.append(max_val)\n",
    "\n",
    "max_score = np.max(score_list)\n",
    "    # score_idx_list.append(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aad3dd-2263-4882-947b-d68296aa7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_mapping(source_ele, r_dicts, r_dicts_alt, window, mask_list, check_thres):\n",
    "  \n",
    "  # print(source)\n",
    "  labels = []\n",
    "  # label_space =  list(r_dicts.keys())\n",
    "  ratio_thres = 0.5\n",
    "  # if ratio_score is too low --> subwording --> lose too much information\n",
    "  \n",
    "  # source_ele = source#.split(' ')\n",
    "  source_len = len(source_ele)\n",
    "  idx = 0\n",
    "    \n",
    "  new_source_ele = []\n",
    "  new_mask_list = []\n",
    "  mask_shift = 0\n",
    "  for i in range(len(source_ele)):\n",
    "        # print(source)\n",
    "        source = source_ele[i]\n",
    "        if '#' in source:\n",
    "            source_list = source.split('#')\n",
    "            # print(source_list)\n",
    "            ratio_score = len(source_list[0])/len(''.join(source_list))\n",
    "            # print(ratio_score)\n",
    "            if ratio_score < ratio_thres:\n",
    "                source = ''.join(source_list)\n",
    "            else:\n",
    "                source = source_list[0]\n",
    "            # print(ratio_score)\n",
    "            # print(source)\n",
    "        if ' ' in source:\n",
    "            new_source_ele += source.split()\n",
    "            new_mask_list.append(mask_list[i-mask_shift])\n",
    "            new_mask_list.append('F')\n",
    "            mask_shift += 1\n",
    "        else:\n",
    "            new_source_ele.append(source)\n",
    "            new_mask_list.append(mask_list[i-mask_shift])\n",
    "        # new_source_ele.append(source)\n",
    "  source_ele = new_source_ele\n",
    "    # raise ValueError(1)\n",
    "    \n",
    "  name_list = ['Anatomical-structure', 'Body-function-and-measurement', 'Medical-condition', 'Medical-device', 'Medical-procedure', 'Medication']\n",
    "        \n",
    "  res_dict = {}\n",
    "\n",
    "  for i in name_list:\n",
    "    res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "    # res = expand_item_pool(r_dicts[i], r_dicts_alt[i])\n",
    "    # print(res[0])\n",
    "    # raise ValueError('1')\n",
    "    # print(len(res[1]))\n",
    "    # print(len(res[2]))\n",
    "    # print('-'*10)\n",
    "    res_dict[i] = res\n",
    "\n",
    "  # raise ValueError('1')\n",
    "  mask_list[0]\n",
    "  while idx < source_len:\n",
    "    if mask_list[idx] == 'T':\n",
    "      if (source_len - idx) >= window:\n",
    "        step = window\n",
    "      else:\n",
    "        step = source_len - idx\n",
    "      candidate_idx = []\n",
    "      candidate_value = []\n",
    "      for shift in range(1, step+1):\n",
    "        term = source_ele[idx:idx + shift]\n",
    "        term_str = ' '.join(term)\n",
    "        # !!! original !!!\n",
    "        # _, candidate = candidates(term_str, r_dicts)\n",
    "        # _, candidate_alt = candidates(term_str, r_dicts_alt)\n",
    "        # s1 = cal_score(term_str, candidate)\n",
    "        # s2 = cal_score(term_str, candidate_alt)\n",
    "        # max_idx, max_value = label_assign(s1, s2)\n",
    "        # !!! no accelerating !!!\n",
    "        # source = term_str\n",
    "        # score_list = []\n",
    "        # for i in name_list:\n",
    "        #     res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "        #     max_val = 0\n",
    "        #     # max_idx = 0\n",
    "        #     for j in range(len(res)):\n",
    "        #         item = res[j]\n",
    "        #         if fuzz.ratio(source, item) > max_val:\n",
    "        #             max_val = fuzz.ratio(source, item)\n",
    "        #             # max_idx = j\n",
    "        #     score_list.append(max_val)\n",
    "        # !!! accelerating\n",
    "        source = term_str\n",
    "        score_list = []\n",
    "\n",
    "        for k, v in res_dict.items():\n",
    "            max_val = 0\n",
    "            # max_idx = 0\n",
    "            # print(v)\n",
    "            # raise ValueError(1)\n",
    "            # fir_tok = source[0]\n",
    "            # print(v[0])\n",
    "            \n",
    "            # if fir_tok in v[0]:\n",
    "            # -----------------------\n",
    "            # code below --> map tokens in phrase\n",
    "            if len(source.split()) == 1:\n",
    "                if source[0] in list(v[0].keys()):\n",
    "                    v_list = v[0][source[0]]\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            elif len(source.split()) == 2:\n",
    "                if source[0] in list(v[1].keys()):\n",
    "                    v_list = v[1][source[0]]\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            elif len(source.split()) == 3:\n",
    "                if source[0] in list(v[2].keys()):\n",
    "                    v_list = v[2][source[0]]\n",
    "                    # print(v_list)\n",
    "                    # raise ValueError(1)\n",
    "                else:\n",
    "                    v_list = set()\n",
    "            v_list_ = list(v_list)\n",
    "            # -----------------------\n",
    "            # code below --> pooling all phrases\n",
    "            # v_list_ = \n",
    "            # for j in range(len(v_list_)):\n",
    "            #     item = v_list_[j]\n",
    "            #     item_list = item.split()\n",
    "#             source_list = source.split()\n",
    "#             # print(source)\n",
    "#             # map 1 token; 2 token; 3 token\n",
    "#             sum_max_val = 0\n",
    "#             for z in range(len(source_list)):\n",
    "#                 source_tok = source_list[z]\n",
    "#                 if source_tok[0] in list(v.keys()):\n",
    "#                     item_list = v[source_tok[0]]\n",
    "#                 else:\n",
    "#                     item_list = []\n",
    "#                 # print(item_list)\n",
    "#                 # raise ValueError(1)\n",
    "#                 tmp_max_val = 0\n",
    "#                 for item_tok in item_list:\n",
    "#                     # print(source_list)\n",
    "#                     # print(source_tok)\n",
    "#                     # print(item_list)\n",
    "#                     # raise ValueError(1)\n",
    "#                     # for zz in \n",
    "#                     map_len = int(len(source_tok)*check_thres)\n",
    "#                     # print(item)\n",
    "#                     if len(item_tok) < map_len:\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         # if fuzz.ratio(source, item) > max_val:\n",
    "#                         #     max_val = fuzz.ratio(source, item)\n",
    "#                         if source_tok[:map_len] == item_tok[:map_len]:\n",
    "#                             # print(source[:map_len])\n",
    "#                             # print(item)\n",
    "#                             # print(source_tok)\n",
    "#                             # print(item_tok)\n",
    "#                             # raise ValueError(1)\n",
    "#                             IoU_score = map_len/len(item_tok)\n",
    "#                             # print(IoU_score)\n",
    "#                             # raise ValueError(1)\n",
    "#                             if IoU_score > sum_max_val:\n",
    "#                                 tmp_max_val = IoU_score\n",
    "#                 sum_max_val += tmp_max_val\n",
    "#             sum_max_val = sum_max_val#/len(source_list)\n",
    "#             if sum_max_val > max_val:\n",
    "#                 max_val = sum_max_val\n",
    "                \n",
    "#                 # fuzz_score = fuzz.ratio(source, item)\n",
    "#                 # # print(source)\n",
    "#                 # # print(item)\n",
    "#                 # # raise ValueError('1')\n",
    "#                 # if fuzz_score > max_val:\n",
    "#                 #     max_val = fuzz_score\n",
    "#                 #     # max_idx = j\n",
    "#             # score_list.append(max_val/len(source_list))\n",
    "#             score_list.append(max_val)\n",
    "            # -------------------------\n",
    "        # print(source)\n",
    "        # print('_'*10)\n",
    "        # print(score_list)\n",
    "        # raise ValueError(1)\n",
    "            # map_thres = 0.9\n",
    "            # print(source)\n",
    "            # map_len = int(len(source)*check_thres)\n",
    "            # print(map_len)\n",
    "            # print(source[:map_len])\n",
    "            # print(len(source.split()))\n",
    "            # raise ValueError(1)\n",
    "            for j in range(len(v_list_)):\n",
    "                item = v_list_[j]\n",
    "                item_list = item.split()\n",
    "                source_list = source.split()\n",
    "                # map 1 token; 2 token; 3 token\n",
    "                sum_max_val = 0\n",
    "                # print(source_list)\n",
    "                # print(item_list)\n",
    "                # raise ValueError(1)\n",
    "                for z in range(len(source_list)):\n",
    "                    source_tok = source_list[z]\n",
    "                    item_tok = item_list[z]\n",
    "                    # try:\n",
    "                    #     source_tok = source_list[z]\n",
    "                    #     item_tok = item_list[z]\n",
    "                    # except IndexError:\n",
    "                    #     print(z)\n",
    "                    #     print(source_list)\n",
    "                    #     print(item_list)\n",
    "                    map_len = int(len(source_tok)*check_thres)\n",
    "                    # print(item)\n",
    "                    if len(item_tok) < map_len:\n",
    "                        break\n",
    "                    else:\n",
    "                        # if fuzz.ratio(source, item) > max_val:\n",
    "                        #     max_val = fuzz.ratio(source, item)\n",
    "                        # raise ValueError(1)\n",
    "                        if source_tok[:map_len] == item_tok[:map_len]:\n",
    "                            # print(source[:map_len])\n",
    "                            # print(item)\n",
    "                            IoU_score = map_len/len(item)\n",
    "                            if IoU_score > sum_max_val:\n",
    "                                sum_max_val = IoU_score\n",
    "                                \n",
    "                if sum_max_val > max_val:\n",
    "                    max_val = sum_max_val\n",
    "                \n",
    "                # fuzz_score = fuzz.ratio(source, item)\n",
    "                # # print(source)\n",
    "                # # print(item)\n",
    "                # # raise ValueError('1')\n",
    "                # if fuzz_score > max_val:\n",
    "                #     max_val = fuzz_score\n",
    "                #     # max_idx = j\n",
    "            score_list.append(max_val/len(source_list))\n",
    "        # raise ValueError(1)\n",
    "        # print(source)\n",
    "        # print(source)\n",
    "        # print(score_list)\n",
    "        # print(term_str)\n",
    "        # score_list = []\n",
    "        # for i in label_space:\n",
    "        #     res = expand_item(r_dicts[i], r_dicts_alt[i])\n",
    "        #     max_val = 0\n",
    "        #     # max_idx = 0\n",
    "        #     for j in range(len(res)):\n",
    "        #         item = res[j]\n",
    "        #         if fuzz.ratio(term_str, item) > max_val:\n",
    "        #             max_val = fuzz.ratio(source, item)\n",
    "        #             # max_idx = j\n",
    "        #     score_list.append(max_val)\n",
    "        # print(score_list)\n",
    "        # max_score = np.max(score_list)\n",
    "        candidate_idx.append(np.argmax(score_list))\n",
    "        candidate_value.append(np.max(score_list))\n",
    "        \n",
    "      # raise ValueError(1)\n",
    "      if np.max(np.array(candidate_value)) > 0.5:\n",
    "        # the position of best matching\n",
    "        max_pos = np.argmax(np.array(candidate_value))\n",
    "        # the label index of best matching\n",
    "        label_idx = candidate_idx[max_pos]\n",
    "        win_shift = max_pos + 1\n",
    "        idx += win_shift\n",
    "        label = name_list[label_idx]\n",
    "        if win_shift == 1:\n",
    "          labels.append('B-' + label)\n",
    "        else:\n",
    "          labels.append('B-' + label)\n",
    "          for i in range(win_shift - 1):\n",
    "            labels.append('I-' + label)\n",
    "      else:\n",
    "        labels.append('O')\n",
    "        idx += 1\n",
    "    else:\n",
    "      labels.append('O')\n",
    "      idx += 1\n",
    "\n",
    "  return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab03af-6782-4d8b-b288-c9dc6a04519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 5223\n",
    "a = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d9462-70bb-49db-a70d-6707fb819ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mask[s_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfc951-1f5f-4baa-88d3-86831824841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemma[s_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8854b2-a75d-4fd8-b381-76fdfb2a9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d995f-935f-44ba-ba1a-8f4522d19ce9",
   "metadata": {
    "gather": {
     "logged": 1668764509852
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    start_time = time.time()\n",
    "    sets = 'KIR'\n",
    "    input_path = '../data/lemma_data/{}/full_sample.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        # print(labels_tmp)\n",
    "        # raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    end_time = time.time()\n",
    "    with open(label_path.replace('label', 'time'), 'w+') as f:\n",
    "        f.write(str(end_time - start_time))\n",
    "        f.close()\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4568f-0e5d-451e-8bd5-17b7927f78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemma[5223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeffb0-5919-4b67-b1a4-05176ef919f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mask[5223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ed8de-4b0a-4e27-9a4e-9d8b2fd21c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(origin[5223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90fdfd-02a0-468d-9d80-18ad4601baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin[5223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6b9d3-65de-4544-abf6-ea3f94c7095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mask[5223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc036b-0f75-46cc-819b-eff477868035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    sets = 'RTG'\n",
    "    input_path = '../data/lemma_data/{}/full_sample.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    # raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        # print(labels_tmp)\n",
    "        # raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747094cb-8a6b-4c11-bc56-4050f8595760",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    sets = 'SAD'\n",
    "    input_path = '../data/lemma_data/{}/full_sample.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    # raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        # print(labels_tmp)\n",
    "        # raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f70e7e-dfe3-42e1-bfd4-e36e9d2b4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    sets = 'LAH'\n",
    "    input_path = '../data/lemma_data/{}/full_sample.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    # raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        # print(labels_tmp)\n",
    "        # raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac12231-af0f-440a-b1e2-6c500849f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # obtain labeling dicts\n",
    "    sets = 'OPER'\n",
    "    input_path = '../data/lemma_data/{}/full_sample.conllu'.format(sets)\n",
    "    mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "    sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "    label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "    mesh_dict = Mesh_dict(mesh_path)\n",
    "    rel_list = Estab_rel(mesh_dict)\n",
    "    window = 3\n",
    "    # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "    res_dict, _ = Build_graph(rel_list)\n",
    "    flat_dict = Flat_dict(res_dict)\n",
    "    anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "    # create res dict with label lakes\n",
    "    # res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "    res_dicts, res_dicts_alt = reverse_dict()\n",
    "    origin, lemma, mask, case, pos = lemma_tokens(input_path)\n",
    "    # origin, lemma, mask = lemma_tokens(input_path)\n",
    "    \n",
    "    # raise ValueError('1')\n",
    "\n",
    "    labels_list = []\n",
    "    token_list = []\n",
    "    # print(len(origin))\n",
    "    for s_idx in tqdm(range(len(origin))):\n",
    "        #origin_tmp = ' '.join(origin[s_idx])\n",
    "        #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "        labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx], 1.0)\n",
    "        # labels_tmp = beam_mapping(origin[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "        # print(labels_tmp)\n",
    "        # raise ValueError(1)\n",
    "        # print(' '.join(labels_tmp))\n",
    "        # collection\n",
    "        labels_list.append(labels_tmp)\n",
    "        token_list.append(origin[s_idx])\n",
    "        # if s_idx > 10000:\n",
    "        #     break\n",
    "    # write to the file\n",
    "    write_list(label_path, labels_list)\n",
    "    write_list(sent_path, token_list)\n",
    "    # print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcdccf-23fb-4853-8a2b-7b1a19386a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     # obtain labeling dicts\n",
    "#     sets = 'test'\n",
    "#     input_path = '../data/lemma_data/{}/1.conllu'.format(sets)\n",
    "#     mesh_path = '../tools/fin-mesh/mesh-skos.ttl'\n",
    "#     sent_path = '../data/processed_data/{}/full_sample_anno.txt'.format(sets)\n",
    "#     label_path = '../data/processed_data/{}/label.txt'.format(sets)\n",
    "\n",
    "#     mesh_dict = Mesh_dict(mesh_path)\n",
    "#     rel_list = Estab_rel(mesh_dict)\n",
    "#     window = 3\n",
    "#     # mesh:D055754 is the top one, also it is a English term -> remove\n",
    "#     res_dict, _ = Build_graph(rel_list)\n",
    "#     flat_dict = Flat_dict(res_dict)\n",
    "#     anno_dict = create_new_dicts(flat_dict, mesh_dict)\n",
    "#     # create res dict with label lakes\n",
    "#     res_dicts, res_dicts_alt = create_res_dicts(anno_dict, mesh_dict)\n",
    "#     origin, lemma, mask = lemma_tokens(input_path)\n",
    "\n",
    "#     labels_list = []\n",
    "#     token_list = []\n",
    "#     print(len(origin))\n",
    "#     for s_idx in range(len(origin)):\n",
    "#         #origin_tmp = ' '.join(origin[s_idx])\n",
    "#         #lemma_tmp = ' '.join(lemma[s_idx])\n",
    "#         labels_tmp = beam_mapping(lemma[s_idx], res_dicts, res_dicts_alt, window, mask[s_idx])\n",
    "#         # collection\n",
    "#         labels_list.append(labels_tmp)\n",
    "#         token_list.append(origin[s_idx])\n",
    "#         # if s_idx > 10000:\n",
    "#         #     break\n",
    "#     # write to the file\n",
    "#     write_list(label_path, labels_list)\n",
    "#     write_list(sent_path, token_list)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
