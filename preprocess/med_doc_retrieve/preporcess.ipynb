{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd10ff-d61a-4808-bcbd-15d0df94b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a00d13-2dd0-4dc2-a99c-d104b6e60b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python library\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# keep only alphatical and alpha+numerical tokens\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17570544-0f0d-4654-b925-0386298009d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_select_code(df):\n",
    "    # df to store the text and code\n",
    "    df_text_code = df[['teksti', 'nakyma']]\n",
    "    # collect the code --> count code --> distribution\n",
    "    code_list = []\n",
    "    for idx, row in df_text_code.iterrows():\n",
    "        code_list.append(row['nakyma'])\n",
    "    # return the dict = {code: count}\n",
    "    code_num_dict = dict(Counter(code_list))\n",
    "    # based on the clinician's suggestion\n",
    "    # LÄH; KIR; OPER; RTG; SÄD; --> significant to clinilians's works\n",
    "    num_KIR = code_num_dict['KIR']\n",
    "    num_RTG = code_num_dict['RTG']\n",
    "    num_LAH = code_num_dict['LÄH']\n",
    "    num_SAD = code_num_dict['SÄD']\n",
    "    num_OPER = code_num_dict['OPER']\n",
    "\n",
    "    num_list = [num_KIR, num_LAH, num_OPER, num_RTG, num_SAD]\n",
    "    print('KIR:{}, RTG:{}, LAH:{}, SAD:{}, OPER:{}'.format(num_list[0], num_list[1], num_list[2], num_list[3], num_list[4]))\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1e62a-11da-41f5-b7a9-4551ec2bbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_samples(df, num, code_type):\n",
    "    '''\n",
    "    Aim: truncate dataframe by the defined num (given medical specialty)\n",
    "    Input: \n",
    "        df --> sum of dataframe\n",
    "        num --> truncate length\n",
    "        code_type --> medical specialty\n",
    "    Output:\n",
    "        text_collect --> truncated dataframe\n",
    "    '''\n",
    "    text_collect = []\n",
    "    count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['nakyma'] == code_type:\n",
    "            text_collect.append(row['teksti'])\n",
    "            count += 1\n",
    "        if count > num:\n",
    "            break\n",
    "    return text_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eb75c-18a5-468a-afa8-0f0742613d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_file(write_str, output_path):\n",
    "    '''\n",
    "    Aim: write string to file\n",
    "    Input: \n",
    "        write_str --> string to be written\n",
    "        output_path --> write string to the path\n",
    "    Output:\n",
    "        written file\n",
    "    '''\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(write_str)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629f323-929c-4c7f-b5fd-84ac42863b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_main_text(list_item):\n",
    "    '''\n",
    "    Aim: keep the main text of clinical documents\n",
    "    Input:\n",
    "        list_item --> item in the list\n",
    "    Output:\n",
    "        output_item --> list of processed sentences\n",
    "    '''\n",
    "    line_list = []\n",
    "    for line in list_item.split('\\n'):\n",
    "        if len(line.split(' ')) > 5:\n",
    "            line_list += line.split('.')\n",
    "\n",
    "    output_item = [sent.strip() for sent in line_list if len(sent.split(' '))>1]\n",
    "    return output_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc7e65-d827-4124-bee7-09392af565ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(spec_list, processed_data_path, med_spec):\n",
    "    '''\n",
    "    Aim: preprocess the list of medical specialty (remove punctuations; numerical-only tokens; lower all tokens)\n",
    "    Input: \n",
    "        spec_list --> the list of medical specialty \n",
    "        processed_data_path --> the data path of processed_data\n",
    "        med_spec --> medical specialty \n",
    "    '''\n",
    "    med_folder_path = processed_data_path + '/' + med_spec\n",
    "    if not os.path.exists(med_folder_path):\n",
    "        os.mkdir(med_folder_path)\n",
    "    \n",
    "    collect_list = []\n",
    "    for doc in spec_list:\n",
    "        collect_list += keep_main_text(doc)\n",
    "    tokenized_txt_list = []\n",
    "    for sent in tqdm(collect_list):\n",
    "        #print(sent)\n",
    "        tokens = [tok.lower() for tok in tokenizer.tokenize(sent) if not tok.isnumeric()]\n",
    "        if len(tokens) > 10:\n",
    "            tokenized_txt = ' '.join(tokens)\n",
    "            tokenized_txt += ' .'\n",
    "            tokenized_txt_list.append(tokenized_txt)\n",
    "        #print(tokenized_txt)\n",
    "    output_path = med_folder_path + '/full_sample.txt'\n",
    "    write_string_file('\\n'.join(list(set(tokenized_txt_list))), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af468d-788d-4b72-9686-30e1374113ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_list(spec_list, processed_data_path, med_spec):\n",
    "    '''\n",
    "    Aim: preprocess the list of medical specialty (remove punctuations; numerical-only tokens; lower all tokens)\n",
    "    Input: \n",
    "        spec_list --> the list of medical specialty \n",
    "        processed_data_path --> the data path of processed_data\n",
    "        med_spec --> medical specialty \n",
    "    '''\n",
    "    med_folder_path = processed_data_path + '/' + med_spec\n",
    "    if not os.path.exists(med_folder_path):\n",
    "        os.mkdir(med_folder_path)\n",
    "    \n",
    "    collect_list = []\n",
    "    for doc in spec_list:\n",
    "        collect_list += keep_main_text(doc)\n",
    "    tokenized_txt_list = []\n",
    "    for sent in collect_list:\n",
    "        tokens = [tok.lower() for tok in tokenizer.tokenize(sent) if not tok.isnumeric()]\n",
    "        if len(tokens) > 10:\n",
    "            tokenized_txt = ' '.join(tokens)\n",
    "            tokenized_txt += '.'\n",
    "            tokenized_txt_list.append(tokenized_txt)\n",
    "        #print(tokenized_txt)\n",
    "    return tokenized_txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b0203-e68d-4c35-b475-622771443fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = '../data/patient_record_text/patient_record/part-00000-tid-5042385561176658048-dad14b75-56bd-44a4-8a7c-b9b608ed88ea-28513-1-c000.snappy.parquet'\n",
    "df1 = pd.read_parquet(data_path1)\n",
    "KIR_list = retrieve_samples(df1, 30, 'KIR')\n",
    "preprocess(KIR_list, '../data/processed_data/', '1') # num_sentence: 15206  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140cf82-ee56-4caa-87e7-4f714c94e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = '../data/patient_record_text/patient_record/part-00000-.snappy.parquet'\n",
    "data_path2 = '../data/patient_record_text/patient_record/part-00001-.snappy.parquet'\n",
    "data_path3 = '../data/patient_record_text/patient_record/part-00002-.snappy.parquet'\n",
    "data_path4 = '../data/patient_record_text/patient_record/part-00003-.snappy.parquet'\n",
    "data_path5 = '../data/patient_record_text/patient_record/part-00004-.snappy.parquet'\n",
    "data_path6 = '../data/patient_record_text/patient_record/part-00005-.snappy.parquet'\n",
    "data_path7 = '../data/patient_record_text/patient_record/part-00006-.snappy.parquet'\n",
    "\n",
    "df1 = pd.read_parquet(data_path1)\n",
    "df2 = pd.read_parquet(data_path2)\n",
    "df3 = pd.read_parquet(data_path3)\n",
    "df4 = pd.read_parquet(data_path4)\n",
    "df5 = pd.read_parquet(data_path5)\n",
    "df6 = pd.read_parquet(data_path6)\n",
    "df7 = pd.read_parquet(data_path7)\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811d74f-1de8-4592-aeae-a4779d5ef9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7], ignore_index=True)\n",
    "d = dict(Counter(df['nakyma'].tolist()))\n",
    "preprocess(KIR_list, '../data/processed_data/', 'KIR') # num_sentence: 15206  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82639acb-6d45-4a66-b700-11583d5a3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d4d8f-6da3-47b6-b9b6-c263ffd54a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f364c-baed-49f2-a86d-391325648bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show few lines of combined df\n",
    "# check the column --> 'teksti' and 'nakyma' are needed \n",
    "# 'nakyma' is used to separate clinical documents by medical specialty\n",
    "print(df1.columns)\n",
    "df1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4d1f8-3128-49fd-88fd-78474aa42e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep main text of documents\n",
    "line_list = []\n",
    "for line in df1.iloc[4]['teksti'].split('\\n'):\n",
    "    if len(line.split(' ')) > 5:\n",
    "        line_list += line.split('.')\n",
    "        \n",
    "[sent.strip() + '.' for sent in line_list if len(sent)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa2ba6-2094-4dc8-8c10-44f5b351ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_select_code(df1)\n",
    "count_select_code(df2)\n",
    "count_select_code(df3)\n",
    "count_select_code(df4)\n",
    "count_select_code(df5)\n",
    "count_select_code(df6)\n",
    "count_select_code(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0875688-07d9-4548-8ef6-c6f0176fabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6,df7], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee619a7-8d42-4471-861a-941ef8e3a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spe = np.array([18879, 10893, 8029, 21090, 9709])\n",
    "num_spe/np.sum(num_spe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8003458-854b-489f-83e2-7c7e8c01402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "887/(887+1089+802+2109+970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78407c02-5398-44a4-97b0-5b03b5806513",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. concate all dataframe\n",
    "the num of annotations for each medical specialty:\n",
    "KIR: 887\n",
    "RTG: 1089\n",
    "LAH: 802\n",
    "SAD: 2109\n",
    "OPER: 970\n",
    "'''\n",
    "KIR_list = retrieve_samples(df, 3887, 'KIR')\n",
    "RTG_list = retrieve_samples(df, 9089, 'RTG')\n",
    "LAH_list = retrieve_samples(df, 4502, 'LÄH')\n",
    "SAD_list = retrieve_samples(df, 3209, 'SÄD')\n",
    "OPER_list = retrieve_samples(df, 1970, 'OPER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24068253-cacd-4090-b0f8-f062cee4523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(KIR_list, '../data/processed_data/', 'KIR') # num_sentence: 15206  \n",
    "preprocess(RTG_list, '../data/processed_data/', 'RTG') # num_sentence: 9687   \n",
    "preprocess(LAH_list, '../data/processed_data/', 'LAH') # num_sentence: 12019   \n",
    "preprocess(SAD_list, '../data/processed_data/', 'SAD') # num_sentence: 13499   \n",
    "preprocess(OPER_list, '../data/processed_data/', 'OPER') # num_sentence: 22527   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c716c9-bbdc-4924-9c93-8e2c2cd9a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = 'SAD'\n",
    "with open('../../data/processed_data/{}/full_sample.txt'.format(sets), 'r') as f:\n",
    "    print(len(f.readlines()))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52752d10-94ff-4604-888a-6b1c57e67464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoNLL 2002 --> 16.7%\n",
    "num_list = np.array([11289, 10112, 10113, 11100, 10149])\n",
    "num_list/np.sum(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285192d-66b3-4bab-8696-20c5c59b640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = np.round(1000*num_list/np.sum(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64be8c-e542-4a22-9a62-1a11a51b3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4a538-ceb7-4cc8-866f-42002fc59f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(num_sample)):\n",
    "    prop = num_sample[idx]/num_list[idx]\n",
    "    print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e42b36-a52c-45c4-8fcc-a3d8c4be6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotaion rate is too low\n",
    "# thus, we need to select informative samples to establish test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d69697-e461-4416-b977-cf310e1cecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_KIR_list = token_list(KIR_list, '../data/processed_data/', 'KIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706cfcc-95cd-49f1-b060-cfdd6f8e6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_KIR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7dd78-d44e-490c-a581-8ce3a4a55dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e171c7-2844-4601-b993-7ee5444266ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_KIR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954efae-9773-42a2-a512-17b62c23389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf90e6-46bd-4836-bf67-9a45836a4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "res = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d41e4-1999-4387-8aa6-33a4e25a7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571e2a0-ee19-41a4-9709-5ad10eab574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = Kmeans(n_cluster=2, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb6fce-a8a1-45eb-851a-2a2554b0be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e43962-9792-479c-b166-bd4e68c1e038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
