{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee27996-a6f2-4f3d-802c-e42a1e69dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d459a-807d-486c-ad37-843df0c5b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python library\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# keep only alphatical and alpha+numerical tokens\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f2382-44dc-44c8-92bc-e5701ff59e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_file(write_str, output_path):\n",
    "    '''\n",
    "    Aim: write string to file\n",
    "    Input: \n",
    "        write_str --> string to be written\n",
    "        output_path --> write string to the path\n",
    "    Output:\n",
    "        written file\n",
    "    '''\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(write_str)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02926e5d-655c-47df-a49d-86125c048e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(spec_list, processed_data_path, sets):\n",
    "    '''\n",
    "    Aim: preprocess the list of medical specialty (remove punctuations; numerical-only tokens; lower all tokens)\n",
    "    Input: \n",
    "        spec_list --> the list of medical specialty \n",
    "        processed_data_path --> the data path of processed_data\n",
    "    '''\n",
    "    med_folder_path = processed_data_path\n",
    "    if not os.path.exists(med_folder_path):\n",
    "        os.mkdir(med_folder_path)\n",
    "    \n",
    "    collect_list = []\n",
    "    for doc in spec_list:\n",
    "        collect_list += keep_main_text(doc)\n",
    "    tokenized_txt_list = []\n",
    "    for sent in tqdm(collect_list):\n",
    "        tokens = [tok.lower() for tok in tokenizer.tokenize(sent) if not tok.isnumeric()]\n",
    "        if len(tokens) > 1:\n",
    "            tokenized_txt = ' '.join(tokens)\n",
    "            tokenized_txt_list.append(tokenized_txt)\n",
    "        #print(tokenized_txt)\n",
    "    output_path = med_folder_path + '/pretrain_text_{}.txt'.format(sets)\n",
    "    write_string_file('\\n'.join(tokenized_txt_list), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1105b-c0a3-4d1c-a7c8-f701c15af5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_main_text(list_item):\n",
    "    '''\n",
    "    Aim: keep the main text of clinical documents\n",
    "    Input:\n",
    "        list_item --> item in the list\n",
    "    Output:\n",
    "        output_item --> list of processed sentences\n",
    "    '''\n",
    "    line_list = []\n",
    "    for line in list_item.split('\\n'):\n",
    "        if len(line.split(' ')) > 5:\n",
    "            line_list += line.split('.')\n",
    "\n",
    "    output_item = [sent.strip() for sent in line_list if len(sent.split(' '))>1]\n",
    "    return output_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9755c5-4063-4a15-b5cc-02360e5133d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../data/patient_record_pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c52403-6a35-4cfe-8f29-082b0ec0a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be0fc0-6258-415f-bf1e-6f04265cfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6bc62-6319-4819-b20c-480853419f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_eval = np.split(df_sample.sample(frac=1, random_state=42), [int(0.9*len(df_sample))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b980c-189a-49a5-a2af-0547bea9d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = []\n",
    "count = 0\n",
    "for idx, row in df_train.iterrows():\n",
    "    train_text_list.append(row['teksti'])\n",
    "    #tok_list = [tok.lower() for tok in tokenizer.tokenize(row['teksti']) if not tok.isnumeric()]\n",
    "    #count+=1\n",
    "    \n",
    "eval_text_list = []\n",
    "for idx, row in df_eval.iterrows():\n",
    "    eval_text_list.append(row['teksti'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c184294-c7f5-4ad4-81b3-3574b0bc378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(train_text_list, '../../data/pretrain_data/', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d87d6-3f76-4aec-ba1d-ea597a409d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(eval_text_list, '../../data/pretrain_data/', 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04588d80-653a-4c99-a428-b7b2efd1af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
