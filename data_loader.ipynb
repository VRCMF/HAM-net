{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39067dea-b1e2-426d-9013-b9873dfeda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061cb3b-2330-4867-a286-659440ae4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, data_dir, bert_model_dir, args, token_pad_idx=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_len = args.max_len\n",
    "        self.device = args.device\n",
    "        self.seed = args.seed\n",
    "        self.token_pad_idx = 0\n",
    "\n",
    "        tags = self.load_tags()\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "        args.tag2idx = self.tag2idx\n",
    "        args.idx2tag = self.idx2tag\n",
    "        self.tag_pad_idx = self.tag2idx['O']\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "    def load_tags(self):\n",
    "        tags = []\n",
    "        file_path = os.path.join(self.data_dir, 'tags.txt')\n",
    "        with open(file_path, 'r') as file:\n",
    "            for tag in file:\n",
    "                tags.append(tag.strip())\n",
    "        # print(tags)\n",
    "        return tags\n",
    "\n",
    "    def load_sentences_tags(self, sentences_file, tags_file, d):\n",
    "        \"\"\"Loads sentences and tags from their corresponding files. \n",
    "            Maps tokens and tags to their indices and stores them in the provided dict d.\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        tags = []\n",
    "\n",
    "        with open(sentences_file, 'r') as file:\n",
    "            for line in file:\n",
    "                # replace each token by its index\n",
    "                #tokens = self.tokenizer.tokenize(line.strip())\n",
    "                tokens = line.strip().split(' ')\n",
    "                sentences.append(self.tokenizer.convert_tokens_to_ids(tokens))\n",
    "        # print(self.tag2idx)\n",
    "        with open(tags_file, 'r') as file:\n",
    "            for line in file:\n",
    "                # replace each tag by its index\n",
    "                tag_seq = [self.tag2idx.get(tag.replace('-&-', '-and-')) for tag in line.strip().split(' ')]\n",
    "                tags.append(tag_seq)\n",
    "                # print(line)\n",
    "                # print(tag_seq)\n",
    "\n",
    "        # checks to ensure there is a tag for each token\n",
    "        #print(len(sentences))\n",
    "        #print(len(tags))\n",
    "        assert len(sentences) == len(tags)\n",
    "        #for i in range(len(sentences)):\n",
    "        #    if len(tags[i]) != len(sentences[i]):\n",
    "        #      print(i)\n",
    "        #      print('\\n')\n",
    "        #      print(tags[i])\n",
    "        #      print(len(tags[i]))\n",
    "        #      print('\\n')\n",
    "        #      print(sentences[i])\n",
    "        #      print(len(sentences[i]))\n",
    "        #      print('\\n')\n",
    "        #    assert len(tags[i]) == len(sentences[i])\n",
    "\n",
    "        # storing sentences and tags in dict d\n",
    "        d['data'] = sentences\n",
    "        d['tags'] = tags\n",
    "        d['size'] = len(sentences)\n",
    "        \n",
    "    def load_data(self, data_type):\n",
    "        \"\"\"Loads the data for each type in types from data_dir.\n",
    "        Args:\n",
    "            data_type: (str) has one of 'train', 'dev', 'test' depending on which data is required.\n",
    "        Returns:\n",
    "            data: (dict) contains the data with tags for each type in types.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        if data_type in ['train', 'dev', 'test']:\n",
    "            sentences_file = os.path.join(self.data_dir, data_type, 'sentences.txt')\n",
    "            tags_path = os.path.join(self.data_dir, data_type, 'tags.txt')\n",
    "            self.load_sentences_tags(sentences_file, tags_path, data)\n",
    "        else:\n",
    "            raise ValueError(\"data type not in ['train', 'dev', 'test']\")\n",
    "        return data\n",
    "\n",
    "    def data_iterator(self, data, shuffle=False):\n",
    "        \"\"\"Returns a generator that yields batches data with tags.\n",
    "        Args:\n",
    "            data: (dict) contains data which has keys 'data', 'tags' and 'size'\n",
    "            shuffle: (bool) whether the data should be shuffled\n",
    "            \n",
    "        Yields:\n",
    "            batch_data: (tensor) shape: (batch_size, max_len)\n",
    "            batch_tags: (tensor) shape: (batch_size, max_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(self.seed)\n",
    "            random.shuffle(order)\n",
    "\n",
    "        # one pass over data\n",
    "        for i in range(data['size']//self.batch_size):\n",
    "            # fetch sentences and tags\n",
    "            sentences = [data['data'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "            tags = [data['tags'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "\n",
    "            # batch length\n",
    "            batch_len = len(sentences)\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_len = max([len(s) for s in sentences])\n",
    "            max_len = min(batch_max_len, self.max_len)\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_idx\n",
    "            batch_data = self.token_pad_idx * np.ones((batch_len, max_len))\n",
    "            batch_tags = self.tag_pad_idx * np.ones((batch_len, max_len))\n",
    "\n",
    "            # copy the data to the numpy array\n",
    "            for j in range(batch_len):\n",
    "                cur_len = len(sentences[j])\n",
    "                if cur_len <= max_len:\n",
    "                    batch_data[j][:cur_len] = sentences[j]\n",
    "                    batch_tags[j][:cur_len] = tags[j]\n",
    "                else:\n",
    "                    batch_data[j] = sentences[j][:max_len]\n",
    "                    batch_tags[j] = tags[j][:max_len]\n",
    "\n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
    "            batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            batch_data, batch_tags = batch_data.to(self.device), batch_tags.to(self.device)\n",
    "    \n",
    "            yield batch_data, batch_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe2944-fa66-4856-a05a-f4637ee1a808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
