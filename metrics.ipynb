{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8e3d1-e34c-4f42-8ff7-22aa3a28daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04cba7-2c5e-47ce-a1dc-61fc8746c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Thanks to https://github.com/chakki-works/seqeval\n",
    "Metrics to assess performance on sequence labeling task given prediction\n",
    "Functions named as ``*_score`` return a scalar value to maximize: the higher\n",
    "the better\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(seq, suffix=False):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "\n",
    "    Example:\n",
    "        >>> from seqeval.metrics.sequence_labeling import get_entities\n",
    "        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
    "        >>> get_entities(seq)\n",
    "        [('PER', 0, 1), ('LOC', 3, 3)]\n",
    "    \"\"\"\n",
    "    # for nested list\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        if suffix:\n",
    "            tag = chunk[-1]\n",
    "            type_ = chunk.split('-')[0]\n",
    "        else:\n",
    "            tag = chunk[0]\n",
    "            type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i-1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk ended between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_end: boolean.\n",
    "    \"\"\"\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba94d71-d057-4c26-9173-d8a240bbde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk started between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_start: boolean.\n",
    "    \"\"\"\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def eval_score(y_true, y_pred, average='micro', digits=2, suffix=False):\n",
    "    \"\"\"Compute the F1 score.\n",
    "\n",
    "    The F1 score can be interpreted as a weighted average of the precision and\n",
    "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    The relative contribution of precision and recall to the F1 score are\n",
    "    equal. The formula for the F1 score is::\n",
    "\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        >>> from seqeval.metrics import f1_score\n",
    "        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> f1_score(y_true, y_pred)\n",
    "        0.50\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return p, r, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8f292-6b82-4c2c-a060-82fe23d2b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Accuracy classification score.\n",
    "\n",
    "    In multilabel classification, this function computes subset accuracy:\n",
    "    the set of labels predicted for a sample must *exactly* match the\n",
    "    corresponding set of labels in y_true.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        >>> from seqeval.metrics import accuracy_score\n",
    "        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> accuracy_score(y_true, y_pred)\n",
    "        0.80\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in y_true):\n",
    "        y_true = [item for sublist in y_true for item in sublist]\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "    nb_correct = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    nb_true = len(y_true)\n",
    "\n",
    "    score = nb_correct / nb_true\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3907d-4991-40f4-aeda-c0f6d98c9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(y_true, y_pred, digits=2, suffix=False):\n",
    "    \"\"\"Build a text report showing the main classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a classifier.\n",
    "        digits : int. Number of digits for formatting output floating point values.\n",
    "\n",
    "    Returns:\n",
    "        report : string. Text summary of the precision, recall, F1 score for each class.\n",
    "\n",
    "    Examples:\n",
    "        >>> from seqeval.metrics import classification_report\n",
    "        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> print(classification_report(y_true, y_pred))\n",
    "                     precision    recall  f1-score   support\n",
    "        <BLANKLINE>\n",
    "               MISC       0.00      0.00      0.00         1\n",
    "                PER       1.00      1.00      1.00         1\n",
    "        <BLANKLINE>\n",
    "        avg / total       0.50      0.50      0.50         2\n",
    "        <BLANKLINE>\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = 'avg / total'\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, true_entities in d1.items():\n",
    "        pred_entities = d2[type_name]\n",
    "        nb_correct = len(true_entities & pred_entities)\n",
    "        nb_pred = len(pred_entities)\n",
    "        nb_true = len(true_entities)\n",
    "\n",
    "        p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(ps, weights=s),\n",
    "                             np.average(rs, weights=s),\n",
    "                             np.average(f1s, weights=s),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    "    return report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
